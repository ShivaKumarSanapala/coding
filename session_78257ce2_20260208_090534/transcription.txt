# Live Transcription Session: session_78257ce2_20260208_090534
# Started: 2026-02-08T09:05:34.587659
# Language: en
# Model: medium
# Accumulation Duration: 5.0s
# Save Audio: No
# ============================================================

[09:05:52] Good morning. I have started sharing my
[09:06:09] screen hope you can see if someone can confirm all is good you can hear me fine awesome thank you thank you folks so let's get started and today's topic is time series modeling RNNs so those are the two main things that I would
[09:06:26] like to do in the span of about three hours. Yeah, right. So we'll get started with time series modeling and then move to the other topic after that. Okay, so let's
[09:06:45] Let's start. Yeah, time series, I mean, I'm sure everyone has seen time series in several places here. So stock market time series, BSE, NSC, NASDAQ, various stock markets that you might be trading in. There are also these
[09:07:01] weather based time series, so temperature time series, precipitation time series and so on. So global land ocean temperature, for example, or other kinds of such. Sorry, if you're not speaking, could you please.
[09:07:19] mute yourselves. So there are basically several such time series. Time series can have different kinds of granularity. So for example, it could be daily. So Monday, Tuesday, Wednesday, Thursday, Friday, that's the granularity at which you could be recording some
[09:07:35] number like number of packets of milk sold or you could have basically you know at an yearly granularity or a 10 yearly granularity like our census time series of population count. So that's a popular time series.
[09:07:50] examples. From a definition perspective, a time series is defined as an ordered sequence of values of a variable at equally spaced time intervals. So ordered means, yes, you have time
[09:08:08] to define the order, so there is 1921, 1931, 41 and so on, right, order of values of a variable. So in these cases, the variables are number of packets of milk sold or population in millions. So basically,
[09:08:24] these are the variables at equally spaced time intervals. So basically, as you can see, this is like one day spacing, this is 10 years spacing, equally spaced time intervals. In fact, this definition is more particularly called as regular
[09:08:43] time series. So, if the time intervals are not equal it is actually called irregular time series. So, if the time intervals are not equal you know maybe you are trying to measure temperature using a faulty thermometer. So, in that case sometimes you get the time series value
[09:08:59] sometimes you don't or in other cases maybe you are measuring every time somebody enters a shop now that probably may not be equally spaced right so then those are called as irregular time series but the ones that are of business
[09:09:14] importance are regular time series, which basically are time series of this kind, where there is equal spacing between different values of the variable that is measured. Now most popular application of time series is to basically
[09:09:30] basically fit a model and do forecasting and monitoring. So essentially the idea is that you observe what's going on with the value of a variable over a period of time and that gives you
[09:09:47] some existing time series, you take that and you fit a model, fit a model in the sense you try to fit a forecasting model in that senses. Mostly forecasting, sometimes you also fit other kinds of model like anomaly detection model.
[09:10:04] So for example if you want to do predictive forecasting about predictive maintenance sorry predictive maintenance then you want to basically figure out hey this medical refrigerator what is the probability that it will go bad in the next one week. So if I basically figure out
[09:10:20] from several sensor time series which are attached to this refrigerator, which medical refrigerators store transplantable organs. So, therefore, it is very important to ensure that you can forecast or predict when it will go bad.
[09:10:39] So if your sensors time series help you come up with a predictive maintenance model, that's really good. So therefore people typically do two kinds of things with time series data, forecasting and monitoring. Monitoring basically meaning monitor how the appliance is working and then based
[09:10:55] the sensors time series and then do predictive maintenance. In this session, we will focus on forecasting application in that sense. And forecasting is applicable in so many domains that you see. So you could be doing economic
[09:11:11] forecasting so forecasting of various economic indicators like for example GDP right you could be doing sales forecasting so you know how many lamps will I sell next Diwali right or or how many milk packets will I sell
[09:11:26] tomorrow and so on. Budgetary analysis, stock market analysis, yield projections, I mean, how many kilograms of food grain will my crop yield this year or for that
[09:11:42] matter any kind of manufacturing sector. In any manufacturing company, you want to do yield projection. Process and quality control, inventory studies, workload projections, many, many of these use cases basically require
[09:11:58] forecasting. Several end applications also depend on forecasting. For example, the entire supply chain management, supply chain kind of studies depend on forecasting. If you can forecast demand, supply, inventory,
[09:12:14] whether the supplier will be able to, what is the probability that they will be able to supply a certain amount. All of those are really forecasting applications. So many, many sectors completely depend on this
[09:12:32] answer is forecasting. So that's about applications and I assume that everyone sort of buys the point that forecasting is important. Anyone who doesn't buy or anyone has a question, feel free to, I'm pausing here, feel free to ask. And again folks, best to best
[09:12:49] basically use the chat window, the Q&A box because that's easiest to then manage the stream of questions. Any questions so far? Anything so far? Hopefully not.
[09:13:06] point that forecasting is important and therefore we should focus on building good forecasting models. By the way, forecasting is not a new thing. Our time series analysis is not a new thing. The earliest time series books have been written almost like four decades back in
[09:13:22] So basically, people have been studying this kind of stuff even before the traditional machine learning started, even before you could basically think about modeling things like machine learning based regression mechanisms and so on, even before that.
[09:13:39] time series analysis has been done. So let's start with the simplest of models. So what I would do in this session is I would start with the simplest of models and then gradually take you to more complicated models.
[09:13:57] At the end, stopping with the RNN based models. Now, the point is that RNNs are not the best models today, which can basically do time series forecasting. In fact, transformer based models can do better time series forecasting.
[09:14:17] today. But that would be some topic that people will cover later. So transformer based models will be covered in another session. But what we would do is to start with the simplest statistical models as they are called and then slowly graduate to deep learning models like RNLs. That's what we are going to
[09:14:35] do in this session today. So let's start with the simplest of models. And for the fun of it, I'm going to use the salary as the time series, right? Salary time series or salary is the variable that you are trying to observe over time. So, I mean, if yt
[09:14:56] is my current salary, what is the salary that I expect in the next year? Yt plus one, that's basically the question that we ask. You see, naive model, the simplest model is the no change forecast model. So the idea is that yt plus one, you can simply set it as yt.
[09:15:15] Now you would observe that at several places I use these hats. So typically in the machine learning or any kind of data science textbook, the hat typically means predicted value. So therefore forecasting is like predicting. So therefore we basically put up a hat there. So the simplest model is that I predict
[09:15:33] salary in the next year to be exactly the same as this year. No hike, no change in that census. Obviously, in India, given that there is inflation rate, which is not zero or negative, we expect basically salary to increase. So nobody likes this kind of a model.
[09:15:52] forecast model where you simply set the predicted value the forecasted value same as the current value or the previous value depending on how you look at it. So what people expect is an upward trend or a downward trend I mean you know for salary yes upward trend if I think about
[09:16:07] about other time series like temperature, measured every hour, then you would expect a downward trend when you go from afternoon to night. So starting from afternoon to night, you would expect that the temperature goes down.
[09:16:24] time series basically expect a trend. So intuitively we expect that it would be an overall upward trend or a downward trend. Now not necessarily that every element in the time series has to basically go always up up up and up you know sometimes you may perform not
[09:16:41] well because you had a life change event. But then you expect your salary typically to go upward when measured over several years. You expect your sales to also typically go up when measured over several years, given that the population is increasing and
[09:16:58] hopefully you do well in business, you would want the sales to go up as well. So we expect trend. Now think about it, how do we really think about trend intuitively, without using any data science tools or whatever, how do we think about trend? When you think about
[09:17:17] your salary right you compare it with your previous year salary and you typically say oh I got like a 10% hike yeah right so we typically say that I got a 10% hike or we typically say that you know I don't know maybe you want to call it like I got a
[09:17:33] 5 lakh hike, 10% hike, 5 lakh hike. So most people say percent but some people also say oh you know I got like 5 lakhs hike. So the trend can be basically, trend is equivalent to hike.
[09:17:52] what we call as hike in typical terms, it's basically equivalent. It's just that hike typically people mention in a context of two time points. So people typically, when they say hike, they typically just mean previous time point and this time point. But well, I mean, the term can be
[09:18:10] And you could say that, hey, I've gotten a hike of 100% over the past five years. So people do that as well. And therefore, hike and trend are similar terms in that sense broadly. So as I was saying, you could get a five-lakh hike.
[09:18:28] you could get a 10% hike. Now the two ways of thinking about it are called as additive trend or multiplicative trend. So basically when you basically talk about hike from an additive perspective, so essentially you have your current salary and you look at how much hike
[09:18:50] did I get in the previous year? That's like y t minus y t minus one. y t minus one would mean a salary in the previous year, right? So that's basically that. And I mean, hopefully it was obvious t is time, t is time here. So y t is my current salary, y t minus one is like previous year's salary. This difference is
[09:19:13] So now if it was 5 lakhs, you know, I mean, if I have similar kind of a trend, I would expect the next year's salary to be Yt plus 5 lakhs. That's that. So if I got like 5 lakhs hike last year, I should basically get the same this year. Yeah. Now notice that this is different from the knife forecast model and this is yet another model
[09:19:37] of predicting or forecasting the time series value. I could also have a multiplicative hike. So I could have yt multiplied by yt divided by yt minus one. See this division, yt divided by yt minus one should capture your 10%. So I should be able to capture that idea of getting a 10% hike. And if I got a 10% hike, I would expect to get a similar 10% hike this year also. Therefore,
[09:19:56] take the current salary, multiply it with 10% and that gives me my forecasted next year salary. So you see we have already talked about broadly two models. One is the naive forecast model where you basically, it's a no change forecast. And the second one is
[09:20:16] this trend-based time series forecasting model? Editive or multiplicative? So there are three models, in fact, that we've talked about so far. I mean, in the sense that if you give me a time series, I would already give you three forecasted values of different nature. Which is the best one? Which of them is
[09:20:36] going to be the best one you would question. So which is the best one? Now see the point is that given a time series which one works well depends on the time series and depends on the data. So you see in some time series maybe the multiplicative hike model works better but in other time series maybe the no change forecast actually works better.
[09:20:54] So therefore, it is important to try to take your time series data, try to divide it into train, validation, test, try out different models on the validation set, try to figure out which of them leads to the best accuracy and use that model.
[09:21:12] So you basically train the model on your train set, evaluate accuracy on the validation set, whichever model gives you the best results, you take that model and you basically deploy that model for your application. This means that you need training data. Now, what does training data mean?
[09:21:29] training data just means the observed time series so far. That's what it means. Training data just means observed time series values so far. When I say you will measure accuracy on validation set, what kind of accuracy would you measure? That's another question.
[09:21:51] The kind of accuracy that you would measure is going to be, remember this is a regression problem, you are trying to predict a number, so therefore you would measure the same things as you measure in typical regression, mean squared error for example. So whichever gives you the lowest mean squared error, that is the one, that is the model that you would pick up and apply.
[09:22:11] deployed or test data or other deployed basically. So that's that. So far we have talked about three models. I mean, not really very, very interesting models, but some simple models, right? So knife forecast, additive, and multiplicative hike. You might have noticed that these models are pretty, pretty simple in nature.
[09:22:27] So essentially, if I think about an average career in this cohort, on average you probably have 10 years experience. So essentially saying you have a timeline of 10 years and you have salary values over those 10 years.
[09:22:44] have those 10 years worth of salary values. Now I am basically telling you that I will predict yt plus 1 and I basically said that hey I will only use this yt salary to predict that in the naive forecast model.
[09:23:04] So I will only use this one, just this guy and then fortunately in this hike-based stuff, I said at least that I will use the last two values. What are you going to do about the remaining values? You are not going to use them? That's too bad. It's a model which does not really make use of the history.
[09:23:25] at all and just throws away useful data is a bad model. You would rather make use of the entire data that you have. I mean, data is precious, especially when it is like, when it is accurate, right? So it's not, it's basically manually labeled as such, right? Because you know your actual salary value. You're not asking some
[09:23:44] GPD to generate those value values. That's actual values. So you can't lose precious data. But unfortunately, so far we have not considered the history of our time series at all. So essentially we have effectively just made use of the current time point or the previous time point to predict the next time point.
[09:24:01] And our next few models should address that. One way of making use of the entire historical time series, all the 10 years worth of salary to predict the 11th year salary is to do the simple average.
[09:24:20] So basically the idea is that yes, you have these 10 years worth of salary value and you want to predict the 11th year salary. What I would do is to basically just do this, you know, take the average. So essentially take the average. This is a simple average.
[09:24:41] assume everyone understands the sigma sign basically means sum. So you're basically taking the sum over all of those past historical values and then just taking the average. So that's called simple averages. Now nobody likes simple averages over their salary. Do you like it? I mean, especially if you have a 10-year experience,
[09:25:02] I don't think you would love to have your next salary dependent on, you know, the 10 year old salary as well. I mean, taking average of that too much. Especially in India, where the inflation rates are so high, you don't want your salary to be an average of the 10 years salary. I mean, next year's salary to be an average of the 10 years.
[09:25:25] Salvi. 10 years back your Salvi was probably like half or one fourth of what it is today. So you don't want to take that average. But anyway, this is a good model. I mean, sometimes this model is useful. For example, if you're trying to measure the temperature of this room in this minute based on the past 10 minutes. Yeah, I mean, not bad.
[09:25:44] I think if you have a temperature reading for the past 10 minutes, probably then average over that is going to be the temperature in the room right now. So for time series which do not fluctuate too much, I think simple averages can work well. And as I said, there is no particularly a model which is a good model.
[09:26:06] I can't certify that some model will be the best model for your data. You have to try it out on your validation set and figure out. I can only say that over the course of this lecture, we are going to go from simpler models to more complex models, but not necessarily that the complex model is going to be the best fit for your data.
[09:26:24] data, you may be able to model the dynamics in your data using simple models also. Anyway, so the simple average model, basically the idea is that you're going to come up with y t plus one as just an average of all the past 10 values. Sometimes you
[09:26:43] just don't care about the next year's forecast, but next to next year's forecast also. So multi-year forecast as it is called or multi-time point forecast. So for example, what if you want to basically actually compute this, right? I mean, what will be your salary two years and
[09:27:02] So if you want to compute the salary two years henceforth, what you could clearly do is to take this average and then you add, so let's say, so what you could do is you take y hat t plus one. Folks, please put your questions in the Q&A box.
[09:27:22] don't mind please. So essentially you take the forecasted value and you have already these t time points of value. So you know if you wanted to forecast for the 12th year what you would do is you take the 10 year salary, you take the 11th year forecasted salary because you don't have the actual value right now.
[09:27:41] And then you take the average over those 11 values. That's one way of doing it. Now, while that is a way of doing it, it would basically unnecessarily require you to compute this particular part twice, which you don't want to do, especially if your history is long. What if your history is like a million points?
[09:28:00] Sometimes that happens. Sometimes basically maybe you're measuring some stuff and you have a very long history. So in that case, you don't want to compute this sum again or this average again. So therefore, what you could do is to basically do an incremental update to your forecast.
[09:28:17] by essentially using a simple formula, which it does an incremental update to the average. So how does it do that? Well, you basically, this thing multiplied by t, so that basically gives you the sum, the sum part only.
[09:28:36] And that some part essentially, well, if you actually got the predicted value, got the actual value at time t plus one, sure, you can use it. So you would basically just use it and divide by t plus one because now you have 11 years worth of salary rather than 10 years worth of salary.
[09:28:53] So that's that. So essentially what it does is to not compute this y hat t plus one again. So it basically just reuses that sum that you had already computed while forecasting for time point t plus one.
[09:29:11] simple average and I'll take questions. Maybe let's take questions or any discussions you might have. We've discussed very simple models so far but let's take questions. Srinivas has a question. Monitoring means change of inputs. Yeah, monitoring basically typically
[09:29:27] just means you know continuously recording the time series values right and also trying to ensure you know if those time series indicate anything anomalous so as I basically provided on that application right one application is forecasting another application is anomaly detected
[09:29:45] or predictive maintenance. So when you do predictive maintenance, that's practically anomaly detection. You're trying to predict when would this machine go bad so that you can maintain it, so that you can do a maintenance check early in time. So that's what monitoring is.
[09:30:06] means? I mean monitoring means recording the time series values across sensors associated let's say with the device and then trying to do predictive meters. Vinod also had a question. I'm sorry Vinod, but if you could basically just type in the chat window. Oh, you did. Thank you. Yeah. Yeah, I did just now. Thank you. Yes.
[09:30:28] So Vinod has asked a question, where do you get the data? For instance, employees data retrieves from the success factors workday. Then we can do the slice and dice data, then go for train data, test validation to figure out the output type. Yeah, good point. I mean, you could, I mean, you know, again, I'm not talking about data collection part here, but you could get data from several sources.
[09:30:50] So you could basically get it from, as you said, employee data from success factors or workday. You could also basically get data typically from sensors. So a whole bunch of sensors exist today. So for example, medical refrigerators surely have lots of sensors. I mean, 10 plus sensors. Your car has crazy number of sensors. So essentially,
[09:31:10] Fitbit banks have so many sensors. There is so much of time series data that is available. And obviously, you have stock market time series data, which is available via several APIs. So there are many, many sources of this time series data. Thank you. Yeah, awesome. So Venkat Ashok,
[09:31:33] has asked a question, while calculating y t plus two cap, we don't have y t plus one, right? How can we calculate y t plus two? Yeah, absolutely. You're totally right, Venkat Ashok. I mean, if you do not have that, you can't do this calculation. So then what you could do is the following. So you could basically, if you do not have
[09:31:57] y t plus 2, you would basically just do this t times y hat t plus 1. Sorry, if you do not have y t plus 1, this is what you would do. You would use your estimated y t plus 1. So you would use your estimated y t plus 1 and then divide by t plus 1. So that's basically that, which basically would just mean y t plus 1 hat is the same as t plus 2.
[09:32:19] to t plus two's prediction in that sense. But when you get y t plus one, how do you incrementally update so as to come up with a prediction for the next one is what this formula is about. So at time point t plus one, when you essentially get the actual value, how would you update the forecast is basically this. Awesome, so Duryodhan is asking how
[09:32:40] is different from linear regression? Oh, great point. Thank you. It is not. So thank you, right? It is not different from linear regression. And let me actually also talk a little bit more about why you think about it as a linear regression. You see, this is similar to linear regression, given that you are using previous time points as features.
[09:33:03] So, and you're basically trying to regress on the next value. Now, you see to train a linear regression model, you would require some good amount of label data. So, that is one way to think about it, which you can do. I mean, if you have 10 years worth of salary, no problem. You can always divide it into windows of four and then you have maybe like six
[09:33:24] different samples in your data and based on six samples yes you can train some linear regression but you see where I'm going right six samples will probably not give you a very good linear regression fit. So for smaller time series you can't do linear regression and that is why you need such kind of simple models. Obviously linear regression is a great model because it can actually learn those weights associated with
[09:33:45] observations, right? Rather than resetting them to all equal weights in that census. But I mean, it works well when you have lots of data. When you don't have, I mean, all you have is like your ten numbers in your salary. I mean, you can't use linear regression there. Yeah? Okay, so there are probably
[09:34:08] Are there any more questions on the chat window or maybe not? Yeah, yeah, we cannot see on the Q&A. I mean, we cannot see messages on the Q&A box. I think that is by definition in Zoom. That is why I'm reading up questions also, right? So, and I'm in fact also speaking the name of the person who actually asked that question.
[09:34:31] oh some of you can see it that's great I mean because I was told that here on zoom people can't see the Q&A box anyway so I mean if you can see it great if you cannot see it please don't feel anything about it I'm actually talking I am calling out the name of the person who asked and I am also speaking out the question before telling the answer okay so you are not losing anything yeah okay so that's
[09:34:55] that. Okay, so that's that. So we talked about a few models. We talked about simple average. And we also said that, hey, for time series like salary, a simple average is not going to work. Essentially, you don't want to average out based on your past 10 years worth of salary. What you want is probably a moving average. So what does that mean? Maybe you know I'm okay doing an average, but
[09:35:17] just an average over the past two or three years, maybe past three years, fine. I mean, I did not do great this year, so you can take my past three years salary and then basically just compute an average over that. So just take the past three years salary and compute an average over that to predict the next year's salary. So what is moving average? Moving average
[09:35:38] of order k, in this case, as I mentioned, it's order three, is the mean value of the k most recent observations, k most recent observations. So basically, you know, the current one, the previous year, and then the, you know, the previous to previous year, if I'm just considering three, that's it. Yeah. So, and that has been
[09:35:58] basically that and you divide by 3 and that gives you your forecast. Now deciding this k is a problem. I mean how do you decide k is a problem and again you would do the same thing on validation set. You would try to figure out what k works well. So whatever k gives you the least mean squared error is the k there.
[09:36:24] you would go ahead with is the key that you would go ahead. So now, you know, if I'm taking average, then clearly this method is not going to handle trend. Okay, so essentially, you know, if I expected to have upward upward trend, well, I mean, if I'm just taking an average, I am not capturing that hype business at all.
[09:36:49] I'm also not capturing, this model also captures seasonality, and I'll talk more about what that is later, but all of us can relate with the broad notion of seasonality, something repeating after every period in some ways. So this model is again a naive model in that sense is it basically just ensures that cares about the latest few values, latest key values, and that's about it.
[09:37:12] Actually using this example, so you see this is a time series of some 30 time points, 30 weeks. And these are purchases done in those 30 weeks, starting from week one all the way up to week 30. Now what I want to do is to basically for the week 31, I want to predict this value, how many purchases, okay?
[09:37:37] So you see one way is to basically just take the average over everything and then compute the full average. But if we are basically using five week moving average, it's called five week moving average, what I would do is basically you see this is my week number, this is my purchases. The first two columns are the same as the previous slide, just written in a compact format.
[09:38:01] then what I would do is I would basically for the 31st week I would just take the past five values these five values and compute the average. So past five values and compute the average and that's going to be my prediction for 31st week. Why five is the point right why should I fix k equal to five okay so maybe I should fix k equal to three or k equal to ten
[09:38:24] for that matter. So to be able to figure out the right value of k, the way I do that is to really take the entire time series that I have so far and try to use k equal to 5 and compute these predictions. So this column is like predictions. That's predictions. And these predictions have been computed using k equal to 5.
[09:39:11] So, try out with k equal to 5. Does it give you a lower mean squared error compared to k equal to 6 or k equal to 4 or other case? If it gives you lower mean squared error, that is why I use k equal to 5. So, how do I compute this prediction? I mean, just in case if you are wondering what is this 289.8, the calculation is here.
[09:40:09] 289.8 is basically take the first five weeks and then try to predict it for the sixth week. So 289.8 is an average of the first five weeks worth of purchases. And then you see this model is not perfect. So it has some error and that's basically what you can obtain. This is called residual. Residual is basically nothing but actual minus predicted. So 268 minus 289.8 and that gives you your minus 21.8. That's the residual. Now what is mean squared error? Mean squared error basically just means you do residual square and then you keep adding. you can compute me or some other matter. I mean until you have the same number of terms.
[09:41:00] So you basically take the squares of these guys, add them up. And if for k equal to five, this value is the lowest, that is why you use k equal to five. And then you do for five time points in the past, you basically use them into a moving average model. That's basically the standard moving average model. So let's see if there are questions. I believe not. Some people can see the Q&A box, which is very good. So that's nice. Sorry, I just spilled some water. Sorry for that. So let's keep going. So we basically observed that yes, k equal to five is a reasonably good model here.
[09:41:41] that's how we can calculate the forecast using moving average model. So those were all simple models folks, but they do not capture this notion of trend nicely because they essentially, I mean, each of them has some troubles. For example, the no change forecast model cannot capture history at all. The hike based models again don't capture history, just capture the past two points. The full average model actually captures the entire history, but hey, I wanted to be giving at least higher weight to my recent values rather than the older salaries. Moving forecast model does give higher weight to the recent values and lower weight to the older values.
[09:42:26] zero weight to the older values, but then hey, it just gives zero weight to the older values. Now, maybe the past is somewhat important and the recent history is not equally important. So you need a notion of weights. So basically what I want to do is to basically come up with a model such that it gives some high weight to my recent observations and lower weight to the older observations. So what I want to do is to basically use the exponential smoothing model. What is it going to do? It is going to assign exponentially decreasing weights as observations will get older. So as you go in the past, it should
[09:43:10] give lesser and lesser bits. Recent observations should be given relatively higher weight in forecasting than of older observations. And the way you obtain that is basically by using these two equations. It's not obvious what these two equations do and don't worry about it. We'll actually look at them in the next two or three slides. I mean we will go into details about each of them with examples also. So don't worry about it. But just look at them and try to understand what they are. So here the idea is that you are trying to sort of predict y t hat. So we have been talking about y t hat. Forget about this s guy.
[09:43:54] don't bother. So we have been talking about y t hat and all I am trying to do is to estimate y t hat with alpha as the weight to my latest observation and 1 minus alpha as the weight to all of my previous observations. So you see, or rather a smooth value over previous observations, forecasted value over previous observations. It's not like I'm going to give the exact one minus alpha to all the previous observations, but to the forecasted value for the previous observation in that sense. And we'll unroll this and see what this means in the next few slides. So you see, if you think about it as a recursive kind of a thing, you would observe that this hat guy is the predicted value
[09:44:42] y t hat, it's expressed in terms of y hat t minus one, y hat t minus one. And therefore it's like a recursion. So some of you might be able to relate what recursion means. Basically you are calling a function with a smaller value. That's what recursion means. So you're trying to predict y t hat in terms of y t minus one hat. That's that. And since it is a recursion, you must have a base case. Now, people who don't understand the question, don't worry. I mean, you know, if you are predicting the current time point based on the previous time point, then you must stop somewhere and you must stop doing this. I mean, in a backward manner at the first time point itself.
[09:45:32] So the idea is that this particular method of exponential smoothing is basically meant to give higher weightage to your recent observations and slightly lower weightage as you go in the past. And the way it works mathematically is using these two equations. These equations are also called as recurrence relations because they are trying to express y t hat in terms of y t minus one hat. Now as I said, it's trying to give a lower weightage to past observations and a higher weightage to more recent observations in the history. So how much lower is basically controlled by this factor alpha. So the speed at which older responses are dampened is a function
[09:46:34] of this value alpha. So if alpha is close to one, dampening will be very quick. So you can also intuitively observe that if alpha is let's say 0.9, you will give 90% weightage to your latest observation and the remaining 10% to all of your past observations, a combination of your past observations. So if your alpha is close to one, dampening is quick. And when alpha is close to zero, dampening is very slow. So we choose the value of, so again alpha is a hyper parameter. How do you choose alpha? Well, you would choose alpha such that on your validation set or even on your train set for that matter. You basically can obtain lowest MSE. So in time series world, if the data is too scarce, you can tune your parameter.
[09:47:44] even on the train set, because if you just have one time series, and that too it's short one, like maybe just 10, 15 points, it's difficult to divide it to train validation test in that sense. So typically what you do is to do this. I mean, as I showed to you on the moving average model, I mean, it's not like you have like million time points, you just have like 30 time points totally. So just try to estimate K based on your entire data that you have seen. Similarly, you would do the same thing here. that you estimate your mean squared errors, your alpha based on the mean squared error, okay? Because anyway, I mean, these models have nothing that is trained as such, okay? I mean, because this alpha is the only parameter that you have in this model anyway, okay? So you would tune it based on the entire data that is observed so far, okay? Let me show you with an example which will make it concrete. So you basically have this time series. There's time here. There is YT.
[09:49:16] There is this estimated value, okay? So this is your y hat in some ways. It's basically written as s, but you can think about it as y hat. Y hat t plus one, okay? So y hat t plus one, okay? And then the next one is error. And the last one is error squared because to compute MSE or sum of squared errors, you want error squared in that sense, okay? So, and I've written that formula again here. So the formula is here again. St is alpha yt minus one plus one minus alpha st minus one. I'm using S, S is same as y hat. So S is same as y hat. So this is the same formula as on the previous slide. And the base one I did not write. So for the base one, you can think about S2 equal to y1. In the previous slide, I had just written it as y2 hat equal to y1. Same thing. I'm just calling it S here. So the point is that since y hat 2 is same as y1, you see the 71 is copied as it is. So that's your base case. But let's try to understand how is the 70 point
[09:51:04] Please mute yourselves. Yeah, thank you. Okay. Sorry for a few seconds. Yeah. Yeah. Okay, so you're basically talking about the time series, about the exponential smoothing thing. And what were we trying to do? Well, we were basically saying that, hey, we already have Y2 set as, Y2 had set as Y1, right? So Y2 had set as Y1. And why is this the case? Or S2 set as Y1, whichever way you want to call it, because we have S here, we will call it S here, okay? So S2 set is Y1. Why do we do that? Because that's the base of our recurrence. And then how is this 70.9 calculated? Let's basically think about how is 70.9 calculated. It's calculated using this formula. In fact, let me write this formula. We are trying to calculate S3. So let me write it from S3's perspective. It's going to be alpha times Yt minus one. Now I have to fix some alpha to be able to apply this formula. And as it says here, we have fixed an alpha of 0.1. So therefore I will write point 1 here, multiply it with y t minus 1. What is y t minus 1?
[09:52:49] minus 1 is 70 here okay yeah plus 1 minus alpha so if alpha is 0.1 1 minus alpha is 0.9 okay multiplied by s t minus 1 what is s t minus 1 s t minus 1 is s 2 okay s 2 so s 2 is 71 okay let me just write the overall formula here first so s 3 is alpha times y2 okay plus 1 minus alpha times s2 okay and using this formula I have just substituted the values and you have gotten this okay now you know if you do this math you would observe that this is 7 plus you know whatever that one is so 9 1 is a 9, 9 7 is 63 so 7 plus 63.9 and that thing is 70.9 if you add it you will get 70.9 okay So that is how these S values, these estimated values are okay. Now, 17.9 is not accurate. Obviously, you know, it has some error compared to 69, and that's the error. It will basically compute some error. And then you do error squared. So if you do this, you would observe that the sum of squared values is 208.94, adding up all of these. Obviously, your goal is to predict the 13th time points value. But first, you want to figure out what your alpha should be. So you try out different values of alpha. So alpha is between 0 and 1. So you try out 0.1, 0.2, 0.3, all the way up to 0 to 1 basically. Maybe you divide into 10 different ranges. You try those out. So what you observe is that maybe alpha equal to 0.5 gives you the lowest mean squared error. So here the mean squared error is 19 with alpha equal to 0.1. With 0.5, it is 16.2.
[09:54:57] which is better, which is lower, lower the better. So if you find 0.5 gives you 16.29, maybe you want to go further down in the grid. So now you should try out things like 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, those 11 values. And then you would probably observe 0.52 gives you your lowest value. And if you want to go finer in the grid, you could basically create 10 different points around 0.52. And that is how you basically choose your model and then just apply it to figure out the 13th time points y value. So that's basically called exponential smoothing, particularly this model is called as single exponential smoothing because you involve only one parameter called alpha here. There's only one parameter called alpha in this single exponential smoothing. It basically controls the decay in how much importance you give to historical time points. In fact, on the next slide, I'm going to explain this decay part a little bit. So this formula, as it looks like, yeah, intuitively you somewhat understand that dk is, but how? I mean, is it a better way of looking at it? So let's basically look at it here. So what have I done? I have basically written out the same formula in this line, st equal to alpha times yt minus one plus one minus alpha times st minus one. It's the same formula where s2 equal to y1, the base case. Now I want to really show to you that this formula.
[09:57:01] when applied over multiple time points gives higher weightage to recent points and lower weightage to older points. So basically you can see that in fact. So let me start with S4, fourth time points prediction. So I could write it using this formula that way alpha times y3 plus 1 minus alpha times S3. I have just done substitution, no big deal. Then the next line what I have done is to just substitute for S3 using the same formula. So same basic formula. So S3 becomes this alpha times y2 plus 1 minus alpha times S2. And now this S2 guy can be substituted with y1. And that gives me my final S4 formula. So the prediction at the fourth time point is alpha times y3 plus 1 minus alpha times all of that stuff in the square bracket. So now let me show you the effect when I fix alpha as 0.5 and alpha is 0.9. The effect with 0.9 is much more clearer. Let's look at 0.9 first. So if I use 0.9, this part basically becomes 0.9y3, which is what you see here. And then for y2, it is alpha into 1 minus alpha. So remember when alpha is 0.9, 1 minus alpha is 0.1. So therefore, alpha into 1 minus alpha becomes 0.09. 0.1 into 0.3. And then for y1, it is 1 minus alpha into 1 minus alpha, so which basically becomes 0.01. So now one thing is obvious, that yes, with 0.9 as the damping factor, you give like 90% weightage to y3, 9% weightage to y2, and 1% weightage to y1, which is exactly what you want. So the recent time point gets a much higher weightage, older time points get lesser weightage. Now you try to fix alpha equal to 0.5 and you observe this is the trend, 0.5 by 3, 0.25 by 2, 0.25 by 1.
[09:59:13] slower. So essentially you give higher weightage to the higher last time point but then the damping happens slowly and you still give like 25% weightage to y1. So that's that. So alpha basically controls how fast you reduce the overall importance that you give to various time points. Okay, any questions so far? I can take up questions if there are any and then we can. Oh, there are several questions looks like, sorry. Yeah, I should take them now then. Oh, no, no, no. Okay, I forgot to dismiss some of these. Yeah, are there any questions? Feel free to put them. This one I already answered. How is it different? I already answered. Oh, Venkat Ashok asked how we calculated this one. Oh, that's basically just the residual. So you see residual means difference. So I think I probably already talked about it in the sense that after you asked the question. So now how is this one calculated? This is just residual, okay? So residual is basically just the difference. As I said, minus 21.8 is the difference between these two guys. 268 and 289.8 I mean it's not exactly on the same line so therefore you might be confused but you know 289.8 minus 268 or rather 268 minus 289.8 okay so that's minus 21.8 okay so what is dampening? Noor Fatima is asking what is dampening? Dampening means reducing so reducing over time so something dampens means it reduces over time So that's what it means. Another word is attenuating. So that's also called attenuating in the sense reducing over time. That's that. Tarun is asking, there always is a trend in the data and exponential smoothing is not good for data when there is trend. Trend in short term window or overall. Oh, so Tarun, I mean, in fact, this alpha guy does capture some
[10:01:43] notion of trend. I mean, I do agree, it doesn't capture trend very well, but it does capture some notion of trend because it gives like very good importance to your latest time points. So I'm not saying that, well, I mean, this model is 100% perfect, but, and therefore what we would do is basically talk about another model, which will explicitly capture trend for that matter. And then it's actually called as double exponential smoothing to capture trend also nicely. Okay, yeah, so that's that. Saikrishna is asking alpha is some random constant value and it should be a minimum value. Yeah, I mean, you know, that's right. I mean, you try and you need to do trial and error. It's a value between zero and one. And rather than calling it random, it's more like you try out different values using grid search. I mean, as I told you, right, you start by fixing alpha as zero, 0.1, 0.2, 0.3, you try out those 11 values, zero to one at a gap of 0.1. You choose the one that gives you the lowest mean squared error and then you basically explore 10 time points with a lower level of granularity around that one. So 0.5k around, you basically choose 11 values like 0.45, 46, 47, 48 and so on. And then if you basically found 0.52 as the best, then you choose yet another level of granularity and 10 time points around that. If 0.52 was the right one, then you would choose 0.515, 0.516, 0.517 and so on. So that's how you basically choose alpha. Surya Teja is asking, when alpha is less than 0.5, the nearest time point gets lower weightage compared to older samples. Yes, Surya Teja, when alpha is less than 0.5, interesting things happen. But the point is that interesting things happen only for the first few time points. Now, you're right. I mean, if I fix, let's say, point alpha equal to 0.1, what would happen? So if fx alpha is 0.1, yeah, I mean, you know, I'm giving 10% weightage to my nearest time point. But now if you think about your second time point, I'm basically, you know, giving it
