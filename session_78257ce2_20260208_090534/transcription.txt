# Live Transcription Session: session_78257ce2_20260208_090534
# Started: 2026-02-08T09:05:34.587659
# Language: en
# Model: medium
# Accumulation Duration: 5.0s
# Save Audio: No
# ============================================================

[09:05:52] Good morning. I have started sharing my
[09:06:09] screen hope you can see if someone can confirm all is good you can hear me fine awesome thank you thank you folks so let's get started and today's topic is time series modeling RNNs so those are the two main things that I would
[09:06:26] like to do in the span of about three hours. Yeah, right. So we'll get started with time series modeling and then move to the other topic after that. Okay, so let's
[09:06:45] Let's start. Yeah, time series, I mean, I'm sure everyone has seen time series in several places here. So stock market time series, BSE, NSC, NASDAQ, various stock markets that you might be trading in. There are also these
[09:07:01] weather based time series, so temperature time series, precipitation time series and so on. So global land ocean temperature, for example, or other kinds of such. Sorry, if you're not speaking, could you please.
[09:07:19] mute yourselves. So there are basically several such time series. Time series can have different kinds of granularity. So for example, it could be daily. So Monday, Tuesday, Wednesday, Thursday, Friday, that's the granularity at which you could be recording some
[09:07:35] number like number of packets of milk sold or you could have basically you know at an yearly granularity or a 10 yearly granularity like our census time series of population count. So that's a popular time series.
[09:07:50] examples. From a definition perspective, a time series is defined as an ordered sequence of values of a variable at equally spaced time intervals. So ordered means, yes, you have time
[09:08:08] to define the order, so there is 1921, 1931, 41 and so on, right, order of values of a variable. So in these cases, the variables are number of packets of milk sold or population in millions. So basically,
[09:08:24] these are the variables at equally spaced time intervals. So basically, as you can see, this is like one day spacing, this is 10 years spacing, equally spaced time intervals. In fact, this definition is more particularly called as regular
[09:08:43] time series. So, if the time intervals are not equal it is actually called irregular time series. So, if the time intervals are not equal you know maybe you are trying to measure temperature using a faulty thermometer. So, in that case sometimes you get the time series value
[09:08:59] sometimes you don't or in other cases maybe you are measuring every time somebody enters a shop now that probably may not be equally spaced right so then those are called as irregular time series but the ones that are of business
[09:09:14] importance are regular time series, which basically are time series of this kind, where there is equal spacing between different values of the variable that is measured. Now most popular application of time series is to basically
[09:09:30] basically fit a model and do forecasting and monitoring. So essentially the idea is that you observe what's going on with the value of a variable over a period of time and that gives you
[09:09:47] some existing time series, you take that and you fit a model, fit a model in the sense you try to fit a forecasting model in that senses. Mostly forecasting, sometimes you also fit other kinds of model like anomaly detection model.
[09:10:04] So for example if you want to do predictive forecasting about predictive maintenance sorry predictive maintenance then you want to basically figure out hey this medical refrigerator what is the probability that it will go bad in the next one week. So if I basically figure out
[09:10:20] from several sensor time series which are attached to this refrigerator, which medical refrigerators store transplantable organs. So, therefore, it is very important to ensure that you can forecast or predict when it will go bad.
[09:10:39] So if your sensors time series help you come up with a predictive maintenance model, that's really good. So therefore people typically do two kinds of things with time series data, forecasting and monitoring. Monitoring basically meaning monitor how the appliance is working and then based
[09:10:55] the sensors time series and then do predictive maintenance. In this session, we will focus on forecasting application in that sense. And forecasting is applicable in so many domains that you see. So you could be doing economic
[09:11:11] forecasting so forecasting of various economic indicators like for example GDP right you could be doing sales forecasting so you know how many lamps will I sell next Diwali right or or how many milk packets will I sell
[09:11:26] tomorrow and so on. Budgetary analysis, stock market analysis, yield projections, I mean, how many kilograms of food grain will my crop yield this year or for that
[09:11:42] matter any kind of manufacturing sector. In any manufacturing company, you want to do yield projection. Process and quality control, inventory studies, workload projections, many, many of these use cases basically require
[09:11:58] forecasting. Several end applications also depend on forecasting. For example, the entire supply chain management, supply chain kind of studies depend on forecasting. If you can forecast demand, supply, inventory,
[09:12:14] whether the supplier will be able to, what is the probability that they will be able to supply a certain amount. All of those are really forecasting applications. So many, many sectors completely depend on this
[09:12:32] answer is forecasting. So that's about applications and I assume that everyone sort of buys the point that forecasting is important. Anyone who doesn't buy or anyone has a question, feel free to, I'm pausing here, feel free to ask. And again folks, best to best
[09:12:49] basically use the chat window, the Q&A box because that's easiest to then manage the stream of questions. Any questions so far? Anything so far? Hopefully not.
[09:13:06] point that forecasting is important and therefore we should focus on building good forecasting models. By the way, forecasting is not a new thing. Our time series analysis is not a new thing. The earliest time series books have been written almost like four decades back in
[09:13:22] So basically, people have been studying this kind of stuff even before the traditional machine learning started, even before you could basically think about modeling things like machine learning based regression mechanisms and so on, even before that.
[09:13:39] time series analysis has been done. So let's start with the simplest of models. So what I would do in this session is I would start with the simplest of models and then gradually take you to more complicated models.
[09:13:57] At the end, stopping with the RNN based models. Now, the point is that RNNs are not the best models today, which can basically do time series forecasting. In fact, transformer based models can do better time series forecasting.
[09:14:17] today. But that would be some topic that people will cover later. So transformer based models will be covered in another session. But what we would do is to start with the simplest statistical models as they are called and then slowly graduate to deep learning models like RNLs. That's what we are going to
[09:14:35] do in this session today. So let's start with the simplest of models. And for the fun of it, I'm going to use the salary as the time series, right? Salary time series or salary is the variable that you are trying to observe over time. So, I mean, if yt
[09:14:56] is my current salary, what is the salary that I expect in the next year? Yt plus one, that's basically the question that we ask. You see, naive model, the simplest model is the no change forecast model. So the idea is that yt plus one, you can simply set it as yt.
[09:15:15] Now you would observe that at several places I use these hats. So typically in the machine learning or any kind of data science textbook, the hat typically means predicted value. So therefore forecasting is like predicting. So therefore we basically put up a hat there. So the simplest model is that I predict
[09:15:33] salary in the next year to be exactly the same as this year. No hike, no change in that census. Obviously, in India, given that there is inflation rate, which is not zero or negative, we expect basically salary to increase. So nobody likes this kind of a model.
[09:15:52] forecast model where you simply set the predicted value the forecasted value same as the current value or the previous value depending on how you look at it. So what people expect is an upward trend or a downward trend I mean you know for salary yes upward trend if I think about
[09:16:07] about other time series like temperature, measured every hour, then you would expect a downward trend when you go from afternoon to night. So starting from afternoon to night, you would expect that the temperature goes down.
[09:16:24] time series basically expect a trend. So intuitively we expect that it would be an overall upward trend or a downward trend. Now not necessarily that every element in the time series has to basically go always up up up and up you know sometimes you may perform not
[09:16:41] well because you had a life change event. But then you expect your salary typically to go upward when measured over several years. You expect your sales to also typically go up when measured over several years, given that the population is increasing and
[09:16:58] hopefully you do well in business, you would want the sales to go up as well. So we expect trend. Now think about it, how do we really think about trend intuitively, without using any data science tools or whatever, how do we think about trend? When you think about
[09:17:17] your salary right you compare it with your previous year salary and you typically say oh I got like a 10% hike yeah right so we typically say that I got a 10% hike or we typically say that you know I don't know maybe you want to call it like I got a
[09:17:33] 5 lakh hike, 10% hike, 5 lakh hike. So most people say percent but some people also say oh you know I got like 5 lakhs hike. So the trend can be basically, trend is equivalent to hike.
[09:17:52] what we call as hike in typical terms, it's basically equivalent. It's just that hike typically people mention in a context of two time points. So people typically, when they say hike, they typically just mean previous time point and this time point. But well, I mean, the term can be
[09:18:10] And you could say that, hey, I've gotten a hike of 100% over the past five years. So people do that as well. And therefore, hike and trend are similar terms in that sense broadly. So as I was saying, you could get a five-lakh hike.
[09:18:28] you could get a 10% hike. Now the two ways of thinking about it are called as additive trend or multiplicative trend. So basically when you basically talk about hike from an additive perspective, so essentially you have your current salary and you look at how much hike
[09:18:50] did I get in the previous year? That's like y t minus y t minus one. y t minus one would mean a salary in the previous year, right? So that's basically that. And I mean, hopefully it was obvious t is time, t is time here. So y t is my current salary, y t minus one is like previous year's salary. This difference is
[09:19:13] So now if it was 5 lakhs, you know, I mean, if I have similar kind of a trend, I would expect the next year's salary to be Yt plus 5 lakhs. That's that. So if I got like 5 lakhs hike last year, I should basically get the same this year. Yeah. Now notice that this is different from the knife forecast model and this is yet another model
[09:19:37] of predicting or forecasting the time series value. I could also have a multiplicative hike. So I could have yt multiplied by yt divided by yt minus one. See this division, yt divided by yt minus one should capture your 10%. So I should be able to capture that idea of getting a 10% hike. And if I got a 10% hike, I would expect to get a similar 10% hike this year also. Therefore,
[09:19:56] take the current salary, multiply it with 10% and that gives me my forecasted next year salary. So you see we have already talked about broadly two models. One is the naive forecast model where you basically, it's a no change forecast. And the second one is
[09:20:16] this trend-based time series forecasting model? Editive or multiplicative? So there are three models, in fact, that we've talked about so far. I mean, in the sense that if you give me a time series, I would already give you three forecasted values of different nature. Which is the best one? Which of them is
[09:20:36] going to be the best one you would question. So which is the best one? Now see the point is that given a time series which one works well depends on the time series and depends on the data. So you see in some time series maybe the multiplicative hike model works better but in other time series maybe the no change forecast actually works better.
[09:20:54] So therefore, it is important to try to take your time series data, try to divide it into train, validation, test, try out different models on the validation set, try to figure out which of them leads to the best accuracy and use that model.
[09:21:12] So you basically train the model on your train set, evaluate accuracy on the validation set, whichever model gives you the best results, you take that model and you basically deploy that model for your application. This means that you need training data. Now, what does training data mean?
[09:21:29] training data just means the observed time series so far. That's what it means. Training data just means observed time series values so far. When I say you will measure accuracy on validation set, what kind of accuracy would you measure? That's another question.
[09:21:51] The kind of accuracy that you would measure is going to be, remember this is a regression problem, you are trying to predict a number, so therefore you would measure the same things as you measure in typical regression, mean squared error for example. So whichever gives you the lowest mean squared error, that is the one, that is the model that you would pick up and apply.
[09:22:11] deployed or test data or other deployed basically. So that's that. So far we have talked about three models. I mean, not really very, very interesting models, but some simple models, right? So knife forecast, additive, and multiplicative hike. You might have noticed that these models are pretty, pretty simple in nature.
[09:22:27] So essentially, if I think about an average career in this cohort, on average you probably have 10 years experience. So essentially saying you have a timeline of 10 years and you have salary values over those 10 years.
[09:22:44] have those 10 years worth of salary values. Now I am basically telling you that I will predict yt plus 1 and I basically said that hey I will only use this yt salary to predict that in the naive forecast model.
[09:23:04] So I will only use this one, just this guy and then fortunately in this hike-based stuff, I said at least that I will use the last two values. What are you going to do about the remaining values? You are not going to use them? That's too bad. It's a model which does not really make use of the history.
[09:23:25] at all and just throws away useful data is a bad model. You would rather make use of the entire data that you have. I mean, data is precious, especially when it is like, when it is accurate, right? So it's not, it's basically manually labeled as such, right? Because you know your actual salary value. You're not asking some
[09:23:44] GPD to generate those value values. That's actual values. So you can't lose precious data. But unfortunately, so far we have not considered the history of our time series at all. So essentially we have effectively just made use of the current time point or the previous time point to predict the next time point.
[09:24:01] And our next few models should address that. One way of making use of the entire historical time series, all the 10 years worth of salary to predict the 11th year salary is to do the simple average.
[09:24:20] So basically the idea is that yes, you have these 10 years worth of salary value and you want to predict the 11th year salary. What I would do is to basically just do this, you know, take the average. So essentially take the average. This is a simple average.
[09:24:41] assume everyone understands the sigma sign basically means sum. So you're basically taking the sum over all of those past historical values and then just taking the average. So that's called simple averages. Now nobody likes simple averages over their salary. Do you like it? I mean, especially if you have a 10-year experience,
[09:25:02] I don't think you would love to have your next salary dependent on, you know, the 10 year old salary as well. I mean, taking average of that too much. Especially in India, where the inflation rates are so high, you don't want your salary to be an average of the 10 years salary. I mean, next year's salary to be an average of the 10 years.
[09:25:25] Salvi. 10 years back your Salvi was probably like half or one fourth of what it is today. So you don't want to take that average. But anyway, this is a good model. I mean, sometimes this model is useful. For example, if you're trying to measure the temperature of this room in this minute based on the past 10 minutes. Yeah, I mean, not bad.
[09:25:44] I think if you have a temperature reading for the past 10 minutes, probably then average over that is going to be the temperature in the room right now. So for time series which do not fluctuate too much, I think simple averages can work well. And as I said, there is no particularly a model which is a good model.
[09:26:06] I can't certify that some model will be the best model for your data. You have to try it out on your validation set and figure out. I can only say that over the course of this lecture, we are going to go from simpler models to more complex models, but not necessarily that the complex model is going to be the best fit for your data.
[09:26:24] data, you may be able to model the dynamics in your data using simple models also. Anyway, so the simple average model, basically the idea is that you're going to come up with y t plus one as just an average of all the past 10 values. Sometimes you
[09:26:43] just don't care about the next year's forecast, but next to next year's forecast also. So multi-year forecast as it is called or multi-time point forecast. So for example, what if you want to basically actually compute this, right? I mean, what will be your salary two years and
[09:27:02] So if you want to compute the salary two years henceforth, what you could clearly do is to take this average and then you add, so let's say, so what you could do is you take y hat t plus one. Folks, please put your questions in the Q&A box.
[09:27:22] don't mind please. So essentially you take the forecasted value and you have already these t time points of value. So you know if you wanted to forecast for the 12th year what you would do is you take the 10 year salary, you take the 11th year forecasted salary because you don't have the actual value right now.
[09:27:41] And then you take the average over those 11 values. That's one way of doing it. Now, while that is a way of doing it, it would basically unnecessarily require you to compute this particular part twice, which you don't want to do, especially if your history is long. What if your history is like a million points?
[09:28:00] Sometimes that happens. Sometimes basically maybe you're measuring some stuff and you have a very long history. So in that case, you don't want to compute this sum again or this average again. So therefore, what you could do is to basically do an incremental update to your forecast.
[09:28:17] by essentially using a simple formula, which it does an incremental update to the average. So how does it do that? Well, you basically, this thing multiplied by t, so that basically gives you the sum, the sum part only.
[09:28:36] And that some part essentially, well, if you actually got the predicted value, got the actual value at time t plus one, sure, you can use it. So you would basically just use it and divide by t plus one because now you have 11 years worth of salary rather than 10 years worth of salary.
[09:28:53] So that's that. So essentially what it does is to not compute this y hat t plus one again. So it basically just reuses that sum that you had already computed while forecasting for time point t plus one.
[09:29:11] simple average and I'll take questions. Maybe let's take questions or any discussions you might have. We've discussed very simple models so far but let's take questions. Srinivas has a question. Monitoring means change of inputs. Yeah, monitoring basically typically
[09:29:27] just means you know continuously recording the time series values right and also trying to ensure you know if those time series indicate anything anomalous so as I basically provided on that application right one application is forecasting another application is anomaly detected
[09:29:45] or predictive maintenance. So when you do predictive maintenance, that's practically anomaly detection. You're trying to predict when would this machine go bad so that you can maintain it, so that you can do a maintenance check early in time. So that's what monitoring is.
[09:30:06] means? I mean monitoring means recording the time series values across sensors associated let's say with the device and then trying to do predictive meters. Vinod also had a question. I'm sorry Vinod, but if you could basically just type in the chat window. Oh, you did. Thank you. Yeah. Yeah, I did just now. Thank you. Yes.
[09:30:28] So Vinod has asked a question, where do you get the data? For instance, employees data retrieves from the success factors workday. Then we can do the slice and dice data, then go for train data, test validation to figure out the output type. Yeah, good point. I mean, you could, I mean, you know, again, I'm not talking about data collection part here, but you could get data from several sources.
[09:30:50] So you could basically get it from, as you said, employee data from success factors or workday. You could also basically get data typically from sensors. So a whole bunch of sensors exist today. So for example, medical refrigerators surely have lots of sensors. I mean, 10 plus sensors. Your car has crazy number of sensors. So essentially,
[09:31:10] Fitbit banks have so many sensors. There is so much of time series data that is available. And obviously, you have stock market time series data, which is available via several APIs. So there are many, many sources of this time series data. Thank you. Yeah, awesome. So Venkat Ashok,
[09:31:33] has asked a question, while calculating y t plus two cap, we don't have y t plus one, right? How can we calculate y t plus two? Yeah, absolutely. You're totally right, Venkat Ashok. I mean, if you do not have that, you can't do this calculation. So then what you could do is the following. So you could basically, if you do not have
[09:31:57] y t plus 2, you would basically just do this t times y hat t plus 1. Sorry, if you do not have y t plus 1, this is what you would do. You would use your estimated y t plus 1. So you would use your estimated y t plus 1 and then divide by t plus 1. So that's basically that, which basically would just mean y t plus 1 hat is the same as t plus 2.
[09:32:19] to t plus two's prediction in that sense. But when you get y t plus one, how do you incrementally update so as to come up with a prediction for the next one is what this formula is about. So at time point t plus one, when you essentially get the actual value, how would you update the forecast is basically this. Awesome, so Duryodhan is asking how
[09:32:40] is different from linear regression? Oh, great point. Thank you. It is not. So thank you, right? It is not different from linear regression. And let me actually also talk a little bit more about why you think about it as a linear regression. You see, this is similar to linear regression, given that you are using previous time points as features.
[09:33:03] So, and you're basically trying to regress on the next value. Now, you see to train a linear regression model, you would require some good amount of label data. So, that is one way to think about it, which you can do. I mean, if you have 10 years worth of salary, no problem. You can always divide it into windows of four and then you have maybe like six
[09:33:24] different samples in your data and based on six samples yes you can train some linear regression but you see where I'm going right six samples will probably not give you a very good linear regression fit. So for smaller time series you can't do linear regression and that is why you need such kind of simple models. Obviously linear regression is a great model because it can actually learn those weights associated with
[09:33:45] observations, right? Rather than resetting them to all equal weights in that census. But I mean, it works well when you have lots of data. When you don't have, I mean, all you have is like your ten numbers in your salary. I mean, you can't use linear regression there. Yeah? Okay, so there are probably
[09:34:08] Are there any more questions on the chat window or maybe not? Yeah, yeah, we cannot see on the Q&A. I mean, we cannot see messages on the Q&A box. I think that is by definition in Zoom. That is why I'm reading up questions also, right? So, and I'm in fact also speaking the name of the person who actually asked that question.
[09:34:31] oh some of you can see it that's great I mean because I was told that here on zoom people can't see the Q&A box anyway so I mean if you can see it great if you cannot see it please don't feel anything about it I'm actually talking I am calling out the name of the person who asked and I am also speaking out the question before telling the answer okay so you are not losing anything yeah okay so that's
[09:34:55] that. Okay, so that's that. So we talked about a few models. We talked about simple average. And we also said that, hey, for time series like salary, a simple average is not going to work. Essentially, you don't want to average out based on your past 10 years worth of salary. What you want is probably a moving average. So what does that mean? Maybe you know I'm okay doing an average, but
[09:35:17] just an average over the past two or three years, maybe past three years, fine. I mean, I did not do great this year, so you can take my past three years salary and then basically just compute an average over that. So just take the past three years salary and compute an average over that to predict the next year's salary. So what is moving average? Moving average
[09:35:38] of order k, in this case, as I mentioned, it's order three, is the mean value of the k most recent observations, k most recent observations. So basically, you know, the current one, the previous year, and then the, you know, the previous to previous year, if I'm just considering three, that's it. Yeah. So, and that has been
[09:35:58] basically that and you divide by 3 and that gives you your forecast. Now deciding this k is a problem. I mean how do you decide k is a problem and again you would do the same thing on validation set. You would try to figure out what k works well. So whatever k gives you the least mean squared error is the k there.
[09:36:24] you would go ahead with is the key that you would go ahead. So now, you know, if I'm taking average, then clearly this method is not going to handle trend. Okay, so essentially, you know, if I expected to have upward upward trend, well, I mean, if I'm just taking an average, I am not capturing that hype business at all.
[09:36:49] I'm also not capturing, this model also captures seasonality, and I'll talk more about what that is later, but all of us can relate with the broad notion of seasonality, something repeating after every period in some ways. So this model is again a naive model in that sense is it basically just ensures that cares about the latest few values, latest key values, and that's about it.
[09:37:12] Actually using this example, so you see this is a time series of some 30 time points, 30 weeks. And these are purchases done in those 30 weeks, starting from week one all the way up to week 30. Now what I want to do is to basically for the week 31, I want to predict this value, how many purchases, okay?
[09:37:37] So you see one way is to basically just take the average over everything and then compute the full average. But if we are basically using five week moving average, it's called five week moving average, what I would do is basically you see this is my week number, this is my purchases. The first two columns are the same as the previous slide, just written in a compact format.
[09:38:01] then what I would do is I would basically for the 31st week I would just take the past five values these five values and compute the average. So past five values and compute the average and that's going to be my prediction for 31st week. Why five is the point right why should I fix k equal to five okay so maybe I should fix k equal to three or k equal to ten
[09:38:24] for that matter. So to be able to figure out the right value of k, the way I do that is to really take the entire time series that I have so far and try to use k equal to 5 and compute these predictions. So this column is like predictions. That's predictions. And these predictions have been computed using k equal to 5.
[09:39:11] So, try out with k equal to 5. Does it give you a lower mean squared error compared to k equal to 6 or k equal to 4 or other case? If it gives you lower mean squared error, that is why I use k equal to 5. So, how do I compute this prediction? I mean, just in case if you are wondering what is this 289.8, the calculation is here.
[09:40:09] 289.8 is basically take the first five weeks and then try to predict it for the sixth week. So 289.8 is an average of the first five weeks worth of purchases. And then you see this model is not perfect. So it has some error and that's basically what you can obtain. This is called residual. Residual is basically nothing but actual minus predicted. So 268 minus 289.8 and that gives you your minus 21.8. That's the residual. Now what is mean squared error? Mean squared error basically just means you do residual square and then you keep adding. you can compute me or some other matter. I mean until you have the same number of terms.
[09:41:00] So you basically take the squares of these guys, add them up. And if for k equal to five, this value is the lowest, that is why you use k equal to five. And then you do for five time points in the past, you basically use them into a moving average model. That's basically the standard moving average model. So let's see if there are questions. I believe not. Some people can see the Q&A box, which is very good. So that's nice. Sorry, I just spilled some water. Sorry for that. So let's keep going. So we basically observed that yes, k equal to five is a reasonably good model here.
[09:41:41] that's how we can calculate the forecast using moving average model. So those were all simple models folks, but they do not capture this notion of trend nicely because they essentially, I mean, each of them has some troubles. For example, the no change forecast model cannot capture history at all. The hike based models again don't capture history, just capture the past two points. The full average model actually captures the entire history, but hey, I wanted to be giving at least higher weight to my recent values rather than the older salaries. Moving forecast model does give higher weight to the recent values and lower weight to the older values.
[09:42:26] zero weight to the older values, but then hey, it just gives zero weight to the older values. Now, maybe the past is somewhat important and the recent history is not equally important. So you need a notion of weights. So basically what I want to do is to basically come up with a model such that it gives some high weight to my recent observations and lower weight to the older observations. So what I want to do is to basically use the exponential smoothing model. What is it going to do? It is going to assign exponentially decreasing weights as observations will get older. So as you go in the past, it should
[09:43:10] give lesser and lesser bits. Recent observations should be given relatively higher weight in forecasting than of older observations. And the way you obtain that is basically by using these two equations. It's not obvious what these two equations do and don't worry about it. We'll actually look at them in the next two or three slides. I mean we will go into details about each of them with examples also. So don't worry about it. But just look at them and try to understand what they are. So here the idea is that you are trying to sort of predict y t hat. So we have been talking about y t hat. Forget about this s guy.
[09:43:54] don't bother. So we have been talking about y t hat and all I am trying to do is to estimate y t hat with alpha as the weight to my latest observation and 1 minus alpha as the weight to all of my previous observations. So you see, or rather a smooth value over previous observations, forecasted value over previous observations. It's not like I'm going to give the exact one minus alpha to all the previous observations, but to the forecasted value for the previous observation in that sense. And we'll unroll this and see what this means in the next few slides. So you see, if you think about it as a recursive kind of a thing, you would observe that this hat guy is the predicted value
[09:44:42] y t hat, it's expressed in terms of y hat t minus one, y hat t minus one. And therefore it's like a recursion. So some of you might be able to relate what recursion means. Basically you are calling a function with a smaller value. That's what recursion means. So you're trying to predict y t hat in terms of y t minus one hat. That's that. And since it is a recursion, you must have a base case. Now, people who don't understand the question, don't worry. I mean, you know, if you are predicting the current time point based on the previous time point, then you must stop somewhere and you must stop doing this. I mean, in a backward manner at the first time point itself.
[09:45:32] So the idea is that this particular method of exponential smoothing is basically meant to give higher weightage to your recent observations and slightly lower weightage as you go in the past. And the way it works mathematically is using these two equations. These equations are also called as recurrence relations because they are trying to express y t hat in terms of y t minus one hat. Now as I said, it's trying to give a lower weightage to past observations and a higher weightage to more recent observations in the history. So how much lower is basically controlled by this factor alpha. So the speed at which older responses are dampened is a function
[09:46:34] of this value alpha. So if alpha is close to one, dampening will be very quick. So you can also intuitively observe that if alpha is let's say 0.9, you will give 90% weightage to your latest observation and the remaining 10% to all of your past observations, a combination of your past observations. So if your alpha is close to one, dampening is quick. And when alpha is close to zero, dampening is very slow. So we choose the value of, so again alpha is a hyper parameter. How do you choose alpha? Well, you would choose alpha such that on your validation set or even on your train set for that matter. You basically can obtain lowest MSE. So in time series world, if the data is too scarce, you can tune your parameter.
[09:47:44] even on the train set, because if you just have one time series, and that too it's short one, like maybe just 10, 15 points, it's difficult to divide it to train validation test in that sense. So typically what you do is to do this. I mean, as I showed to you on the moving average model, I mean, it's not like you have like million time points, you just have like 30 time points totally. So just try to estimate K based on your entire data that you have seen. Similarly, you would do the same thing here. that you estimate your mean squared errors, your alpha based on the mean squared error, okay? Because anyway, I mean, these models have nothing that is trained as such, okay? I mean, because this alpha is the only parameter that you have in this model anyway, okay? So you would tune it based on the entire data that is observed so far, okay? Let me show you with an example which will make it concrete. So you basically have this time series. There's time here. There is YT.
[09:49:16] There is this estimated value, okay? So this is your y hat in some ways. It's basically written as s, but you can think about it as y hat. Y hat t plus one, okay? So y hat t plus one, okay? And then the next one is error. And the last one is error squared because to compute MSE or sum of squared errors, you want error squared in that sense, okay? So, and I've written that formula again here. So the formula is here again. St is alpha yt minus one plus one minus alpha st minus one. I'm using S, S is same as y hat. So S is same as y hat. So this is the same formula as on the previous slide. And the base one I did not write. So for the base one, you can think about S2 equal to y1. In the previous slide, I had just written it as y2 hat equal to y1. Same thing. I'm just calling it S here. So the point is that since y hat 2 is same as y1, you see the 71 is copied as it is. So that's your base case. But let's try to understand how is the 70 point
[09:51:04] Please mute yourselves. Yeah, thank you. Okay. Sorry for a few seconds. Yeah. Yeah. Okay, so you're basically talking about the time series, about the exponential smoothing thing. And what were we trying to do? Well, we were basically saying that, hey, we already have Y2 set as, Y2 had set as Y1, right? So Y2 had set as Y1. And why is this the case? Or S2 set as Y1, whichever way you want to call it, because we have S here, we will call it S here, okay? So S2 set is Y1. Why do we do that? Because that's the base of our recurrence. And then how is this 70.9 calculated? Let's basically think about how is 70.9 calculated. It's calculated using this formula. In fact, let me write this formula. We are trying to calculate S3. So let me write it from S3's perspective. It's going to be alpha times Yt minus one. Now I have to fix some alpha to be able to apply this formula. And as it says here, we have fixed an alpha of 0.1. So therefore I will write point 1 here, multiply it with y t minus 1. What is y t minus 1?
[09:52:49] minus 1 is 70 here okay yeah plus 1 minus alpha so if alpha is 0.1 1 minus alpha is 0.9 okay multiplied by s t minus 1 what is s t minus 1 s t minus 1 is s 2 okay s 2 so s 2 is 71 okay let me just write the overall formula here first so s 3 is alpha times y2 okay plus 1 minus alpha times s2 okay and using this formula I have just substituted the values and you have gotten this okay now you know if you do this math you would observe that this is 7 plus you know whatever that one is so 9 1 is a 9, 9 7 is 63 so 7 plus 63.9 and that thing is 70.9 if you add it you will get 70.9 okay So that is how these S values, these estimated values are okay. Now, 17.9 is not accurate. Obviously, you know, it has some error compared to 69, and that's the error. It will basically compute some error. And then you do error squared. So if you do this, you would observe that the sum of squared values is 208.94, adding up all of these. Obviously, your goal is to predict the 13th time points value. But first, you want to figure out what your alpha should be. So you try out different values of alpha. So alpha is between 0 and 1. So you try out 0.1, 0.2, 0.3, all the way up to 0 to 1 basically. Maybe you divide into 10 different ranges. You try those out. So what you observe is that maybe alpha equal to 0.5 gives you the lowest mean squared error. So here the mean squared error is 19 with alpha equal to 0.1. With 0.5, it is 16.2.
[09:54:57] which is better, which is lower, lower the better. So if you find 0.5 gives you 16.29, maybe you want to go further down in the grid. So now you should try out things like 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, those 11 values. And then you would probably observe 0.52 gives you your lowest value. And if you want to go finer in the grid, you could basically create 10 different points around 0.52. And that is how you basically choose your model and then just apply it to figure out the 13th time points y value. So that's basically called exponential smoothing, particularly this model is called as single exponential smoothing because you involve only one parameter called alpha here. There's only one parameter called alpha in this single exponential smoothing. It basically controls the decay in how much importance you give to historical time points. In fact, on the next slide, I'm going to explain this decay part a little bit. So this formula, as it looks like, yeah, intuitively you somewhat understand that dk is, but how? I mean, is it a better way of looking at it? So let's basically look at it here. So what have I done? I have basically written out the same formula in this line, st equal to alpha times yt minus one plus one minus alpha times st minus one. It's the same formula where s2 equal to y1, the base case. Now I want to really show to you that this formula.
[09:57:01] when applied over multiple time points gives higher weightage to recent points and lower weightage to older points. So basically you can see that in fact. So let me start with S4, fourth time points prediction. So I could write it using this formula that way alpha times y3 plus 1 minus alpha times S3. I have just done substitution, no big deal. Then the next line what I have done is to just substitute for S3 using the same formula. So same basic formula. So S3 becomes this alpha times y2 plus 1 minus alpha times S2. And now this S2 guy can be substituted with y1. And that gives me my final S4 formula. So the prediction at the fourth time point is alpha times y3 plus 1 minus alpha times all of that stuff in the square bracket. So now let me show you the effect when I fix alpha as 0.5 and alpha is 0.9. The effect with 0.9 is much more clearer. Let's look at 0.9 first. So if I use 0.9, this part basically becomes 0.9y3, which is what you see here. And then for y2, it is alpha into 1 minus alpha. So remember when alpha is 0.9, 1 minus alpha is 0.1. So therefore, alpha into 1 minus alpha becomes 0.09. 0.1 into 0.3. And then for y1, it is 1 minus alpha into 1 minus alpha, so which basically becomes 0.01. So now one thing is obvious, that yes, with 0.9 as the damping factor, you give like 90% weightage to y3, 9% weightage to y2, and 1% weightage to y1, which is exactly what you want. So the recent time point gets a much higher weightage, older time points get lesser weightage. Now you try to fix alpha equal to 0.5 and you observe this is the trend, 0.5 by 3, 0.25 by 2, 0.25 by 1.
[09:59:13] slower. So essentially you give higher weightage to the higher last time point but then the damping happens slowly and you still give like 25% weightage to y1. So that's that. So alpha basically controls how fast you reduce the overall importance that you give to various time points. Okay, any questions so far? I can take up questions if there are any and then we can. Oh, there are several questions looks like, sorry. Yeah, I should take them now then. Oh, no, no, no. Okay, I forgot to dismiss some of these. Yeah, are there any questions? Feel free to put them. This one I already answered. How is it different? I already answered. Oh, Venkat Ashok asked how we calculated this one. Oh, that's basically just the residual. So you see residual means difference. So I think I probably already talked about it in the sense that after you asked the question. So now how is this one calculated? This is just residual, okay? So residual is basically just the difference. As I said, minus 21.8 is the difference between these two guys. 268 and 289.8 I mean it's not exactly on the same line so therefore you might be confused but you know 289.8 minus 268 or rather 268 minus 289.8 okay so that's minus 21.8 okay so what is dampening? Noor Fatima is asking what is dampening? Dampening means reducing so reducing over time so something dampens means it reduces over time So that's what it means. Another word is attenuating. So that's also called attenuating in the sense reducing over time. That's that. Tarun is asking, there always is a trend in the data and exponential smoothing is not good for data when there is trend. Trend in short term window or overall. Oh, so Tarun, I mean, in fact, this alpha guy does capture some
[10:01:43] notion of trend. I mean, I do agree, it doesn't capture trend very well, but it does capture some notion of trend because it gives like very good importance to your latest time points. So I'm not saying that, well, I mean, this model is 100% perfect, but, and therefore what we would do is basically talk about another model, which will explicitly capture trend for that matter. And then it's actually called as double exponential smoothing to capture trend also nicely. Okay, yeah, so that's that. Saikrishna is asking alpha is some random constant value and it should be a minimum value. Yeah, I mean, you know, that's right. I mean, you try and you need to do trial and error. It's a value between zero and one. And rather than calling it random, it's more like you try out different values using grid search. I mean, as I told you, right, you start by fixing alpha as zero, 0.1, 0.2, 0.3, you try out those 11 values, zero to one at a gap of 0.1. You choose the one that gives you the lowest mean squared error and then you basically explore 10 time points with a lower level of granularity around that one. So 0.5k around, you basically choose 11 values like 0.45, 46, 47, 48 and so on. And then if you basically found 0.52 as the best, then you choose yet another level of granularity and 10 time points around that. If 0.52 was the right one, then you would choose 0.515, 0.516, 0.517 and so on. So that's how you basically choose alpha. Surya Teja is asking, when alpha is less than 0.5, the nearest time point gets lower weightage compared to older samples. Yes, Surya Teja, when alpha is less than 0.5, interesting things happen. But the point is that interesting things happen only for the first few time points. Now, you're right. I mean, if I fix, let's say, point alpha equal to 0.1, what would happen? So if fx alpha is 0.1, yeah, I mean, you know, I'm giving 10% weightage to my nearest time point. But now if you think about your second time point, I'm basically, you know, giving it
[10:04:22] you see second time point it would be alpha into 1 minus alpha so 0.1 into 0.9 which is 0.09 right so the first time point gets 10 percentage second one gets 9 percentage And you know, third one, how much does it get? I mean, you see, the idea is that in general, it would mostly be always damping. It's just that your damping will be slower. So it's not like you would actually start giving higher weightage to older time samples. It should be obvious. Let me actually show intuitively why it should be obvious. You see, if you go to older terms, you would actually keep multiplying by either alpha or 1 minus alpha, okay? Alpha or 1 minus alpha. And when you multiply with a number less than 1, it basically always dampens, yeah? It doesn't matter whether you multiply by 0.9 or you multiply by 0.1, you know, if the number is less than 1, it will always dampen. So always you will actually end up giving higher weightage to your current guy. and slightly lower weightage to the older guys. How much lower weightage you give really depends on alpha. So if alpha is 0.9 then it is obvious that you know you are giving 90% weightage to y3 so you obviously think that you will give lesser weightage but when alpha is 0.1 you tend to think oh you are just giving 10% weightage to your current time point a previous time point what about the other time point you would probably end up getting higher weightage no it's always lower always lower okay yeah okay Rakesh Babu is asking, I see in all the methods, targets also predicted calculated based on previous values. So how the accuracy calculated is precise as we don't have real targets to compare? Oh, that's true. So, you know, at test time, nobody is going to give you real targets to compare. So accuracy is always computed on data that you have, right? So you see, you can't compute accuracy on, I mean, you know, if I basically gave you that the value at 13th time point is something you would still ask the same question, right? So, you know, I mean, you can only compute accuracy on on label data. Yeah. Okay. So Krishna is asking alpha, does it all does it auto increment like scheduler learning rate? Oh, okay. So Krishna, no, no, no, this is not. So you fix alpha. Okay. You do not change alpha at all. Okay. In a single exponential smoothing method, you fix alpha and you do not change it at all. Okay. It's not like learning rate schedulers. No, you do not really do anything.
[10:05:23] I mean, you don't change alphas over time. OK? OK. So that's good. Thank you for all the awesome questions. I think we are good. Yeah. I think, yeah, it's a 10 as well. Let's basically, in fact, take a break. We'll meet at 10.15 and then we'll continue. We'll talk about the matrix that you see on the slide, which are obvious, but we'll also talk about double exponential smoothing and then triple exponential smoothing and so on. Okay? So let's take a quick break, though. So let's take a quick break, meet at 10.15, folks. Okay. Okay.
[10:05:42] Green tea is there. Okay.
[10:05:52] Yeah.
[10:06:16] Bye.
[10:06:25] Okay.
[10:06:34] Yeah.
[10:06:53] Thank you.
[10:07:02] you.
[10:07:10] Bye.
[10:08:55] Thank you.
[10:09:04] Thank you.
[10:09:24] Thank you.
[10:09:53] Thank you.
[10:10:07] . .
[10:10:17] Thank you.
[10:11:23] you
[10:11:32] Okay.
[10:12:44] Oh.
[10:12:53] Okay.
[10:13:03] Thank you.
[10:13:12] you
[10:13:22] . .
[10:14:02] Thank you.
[10:14:13] So we are going to have to do one on the other.
[10:14:23] And then.
[10:14:32] Thank you.
[10:14:41] All right.
[10:14:51] . .
[10:15:06] Make sure the plan is good.
[10:15:15] Okay.
[10:15:25] Thank you.
[10:15:34] Okay.
[10:15:43] Okay.
[10:15:52] Thank you.
[10:16:30] Okay, folks, let's resume.
[10:16:41] There is probably a question there.
[10:16:56] I don't know if I should call it long term or short term memory though, but yeah, I mean higher alpha is deep decay.
[10:17:15] Yeah, maybe we can call it short-term memory. You are right. I mean, because higher decay basically just, or rather slower decay just means that you are still focusing on past time points, which is long term in that sense.
[10:17:34] Okay, that's that. No other questions. Let's basically start with, let's resume. So essentially the performance metrics are going to be these that you see here. It's an aggression problem. So we are going to,
[10:17:53] essentially use these standard regression performance matrix mean absolute deviation so which is basically just computing the residual and taking a mod of the residual absolute value of the residual and then if you have n different time points you
[10:18:11] basically stick an average over there. MAPE mean absolute percent error which is basically you take the absolute value of the difference and then divide it by the predicted value that basically is MAPE.
[10:18:39] Then you have the MSC mean squared error, the standard mean squared error, and the standard root mean squared error. So that's basically that. I mean, I think root mean squared error is, these two are the most popular ones, the others, basically some people use them. Okay.
[10:19:13] Now let's talk about double exponential smoothing. So I mean, the previous single exponential smoothing is good, but as Taran mentioned, it does not explicitly take care of trend or the hike part. So can we explicitly take care of that is the point. So yes, we can. And these equations help you do that. So rather the double exponential smoothing helps you do that.
[10:20:10] It is also called as a holds two parameter method. So it is also called as two parameter method because unlike alpha it has a gamma also. So it has basically two parameters alpha and gamma. So if you look at the first equation, this more or less looks like the previous one. S is the smooth value or the predictor value. Equal to alpha times yt minus 1 plus 1 minus alpha times st minus 1. There is this extra term plus bt minus 1 which is there. But otherwise it is more or less the same. So what does this bt
[10:21:11] minus one. So b t minus one is the predicted hike. Let's put it that way. So it's basically the predicted hike. So and you know how do you compute hike? Well you compute hike by taking the differences between the current value and the previous value. So you basically take gamma times s t minus s t minus one. and 1 minus gamma times previous hikes. So bt minus 1. So what is bt? bt is a hike times series infinite census. So while st is your forecasted actual values, your bts are forecasted hike values so to say. Your bt are forecasted hike values. So I mean, I think the meaning of these two equations is also written at the bottom. I just tried to convey the meaning, but it's also written here. So the first smoothing equation ST, you know, adjust directly
[10:22:11] for the trend of the previous period bt-1. So you see I mean beyond what was there already it also adds bt-1 and what does that bt-1 do? Well it adjusts for the hike. So it adds bt-1. This helps in eliminating the lag and brings this ST minus one to the right base of the current value. So basically saying, you know, you observe that this is not BT, okay, this is not BT, this is BT minus one. So what was the hike in the previous thing, you need to add that so as to come up with the right forecast, okay. So add the trend that you basically observed in the previous one, okay. The second equation, the second smoothing thing for BT, you know, that guy updates the trend. So which is expressed as the difference between the last two values, which is basically this. So and you know, in some ways this BT guy does a single exponential smoothing, but to update the trend, but to update the trend. That's the idea.
[10:23:01] So double exponential smoothing, it's also called the holds to parameter method explicitly takes care of the trend by bringing in this new parameter called gamma to update the trend comparing based on the previous observed, the previous trends based on the previous trends. So since you have these two equations, you basically need, I mean, and these are recursions. I mean, these are recursive relations, right? So ST is expressed in terms of ST minus one and BT minus one. While BT is expressed in terms of BT minus one. So there are two recursive functions here, one for S and one for B. So you need a base case therefore. So base case, you see S2 is set as Y1, as simple as that. So that's the same as in the previous slide. S2 is just set as Y1. While this B, now the B can be
[10:23:52] set in various ways. The simplest way to set B, hike, is to basically set it like this. B3 equal to y2 minus y1. What else do you expect to set this, right? I mean, in the sense that, you know, you observe the first two time points, you figure out the hike, and that's the basic hike, in the sense that's the initial hike to start the recursion. So that's how you can set it. The other way to set it is that, hey, the first few time points are not very trustable, so they may have some noise. and therefore what I want to do is to start from the fifth time point and I set the B5 as an average of the hikes that were observed from between time point 2 and 1, 3 and 2 and 4 and 3. That's basically a way of robust way of
[10:24:38] setting the hike by observing three different time points also called as a warm-up period. So I have a warm-up period of four first four time points and I basically observe the hikes and set B5 as the average of the three hikes in that census. The yet another way of doing it is basically this one which is basically like saying hey maybe you know I have a longer warm-up period. and I do not care about individual hikes but I just care about the initial and the later one so maybe I can set like b 11 as you see b 10 minus b 1 divided by 9 because well 9 years have passed by or rather 9 time points I mean the first and the last one the difference is 9 time points so that's basically yet another way of setting b you could use any of the three you don't
[10:25:25] obviously use all three just use one of the three so any of those three can be used just to set the initial b once you have the initial b then clearly you can apply recursion to get the to get the next few b's and but before that yes you have to basically compute st because bt uses st in that senses okay and s you can compute all of them because s2 is set as y1 So for both of the recursions you have your base cases, S2 is set as Y1, B3 is set using any of these three methods, any of these three methods, B3 or B5 or whatever, B11 if you want to. Any questions for this one? I mean this is called double exponential smoothing, it basically
[10:26:12] as you can see inherently encodes the height thing using a new parameter called B. I mean, I'm not showing a full-fledged example here, but nevertheless, there's an example. So consider this data set. So if you have these values measured at various time points, this time you have to basically fit both of these values, alpha and gamma. So you have to try grid search on both of them. So you vary the values. So you take 10 values of alpha, 10 values of gamma. That gives you like total, maybe around 100 combinations. Or if you take like zero as well, so it's like 11 into 11, 121 combinations. You try to figure out which of them give you the lowest
[10:26:52] mean squared error and then you can refine the grid further around those alpha and gamma values which give you the lowest mean squared error at a granularity of 0.1. So that's that and that is how you basically try to figure out whatever alpha and gamma give you the lowest value. You can then compare it with the single exponential smoothing. Remember the method that I talked about before the break? So if your double exponential smoothing somehow on your data gives you lower mean squared error, then you go ahead with double exponential smoothing and compute the next value here. But if your single exponential smoothing gives you lower mean squared error, so be it. Then you actually just do single exponential smoothing and compute the next value.
[10:27:27] So that's that. So one important thing to remember is that your one period ahead forecast FT plus one is given as ST plus BT. Okay. And similarly, M period forecast is given as ST plus M times BT. The idea being that BT is your last hike, right? So now if you do not have observations in the next time points, but you have to come up with a look ahead forecast for the future, not an incremental forecast. No, no. If you are doing incremental forecast, then you would just increment and then you apply the equations nicely and compute the next values. But you are doing a look ahead forecast.
[10:27:56] today tell me what will be the value after two years or after five years then you basically just use this formula then you can assume your hike to be fixed BT and essentially update your FT, update your forecast based on your current ST and BT. M times BT if you are looking five years, M years in advance. Okay, so that's about this folks. Any questions about double exponential smoothing before we go to triple exponential smoothing and other things, right?
[10:28:26] What is the value of b2 base value? No, there is no b2. I mean, you know, you don't start with b2. You basically just start with b3. Okay, you start with b3 and that's basically set to y2 minus y1. You don't compute b2. Venkat Ashok, you don't compute b2. Sir, here in the formula st equal to, we have right bt minus 1, st can be s3. Yes, s3. So, therefore, I mean, if you basically are computing using b3, you don't compute s3.
[10:28:58] at all. The earliest forecast you can come up with is 4. Thank you. Yeah. So thank you. Yeah, that's a great point. Yeah. I mean, but you see the point that if you do not know your hike, you can't compute the estimated value. So you first need to know your hike. And if you're computing hike starting B3, you can only compute estimates fourth year onwards. Okay. That's that. Awesome. Great question. Thank you for being attentive. Yeah. Okay. Taran is asking another one. Less gamma implies higher
[10:29:25] importance to past trend, then less gamma implies higher importance to past trend, then trend between last two observations. Higher gamma historical trend implies historical trend less weightage. more to previous two observations. No, Tarun, that's not true. So see, I mean, gamma and alpha are both following the same way of doing things. Higher gamma basically means your dampening is higher. I mean, always, always you give
[10:29:45] higher importance to recent values, lower importance to old values like in single exponential smoothing. So whether your gamma is less or small, it will still have higher importance to recent values. It's just that the dampening rate is going to be different. So I would rather say that higher gamma basically means that your dampening of previous hikes is
[10:30:02] faster. But if you have less gamut then your dampening of previous hikes is slower.
[10:30:22] Oh, I see. I see your point. Yeah. Good point. So you could say S2 equal to y1 or you could do S3 equal to y2, S4 equal to y3 until you can't compute those. Yeah. Great point. Clear sir. Thank you. Thank you. Yes. Yeah. That's that. So I think I answered
[10:30:48] Tarun's question. Pinakin has a question. Isn't gamma used in warm-up period? No, no, no. Gamma is used always. No, no. Gamma is not used in warm-up period in fact. In warm-up period, you do not want it to be estimated. You want the height to be calculated exactly. So in warm-up period, you don't use gamma. After that, you start using gamma because the warm-up period is only to set
[10:31:08] your initial basic value. Once your initial basic value is set, then you essentially start doing this gamma business so as to essentially estimate it for future time points. Great questions. Thank you, folks. You're thinking quite some, which is good. Let's keep going. Let me just try to hide this guy.
[10:31:26] let's try let's talk about seasonality right so all of us season understand seasonality broadly so it means periodic fluctuations so for example in the US retail sales peak around the Christmas season and decline after the holidays right so that there is seasonality and then similarly this seasonality
[10:31:48] in terms of temperature, right? I mean, in the daytime, it is high, in the nighttime, it is low, and it keeps going, swinging like that, right? So there's seasonality of different types. I mean, there's seasonality sometimes monthly, sometimes weekly, and so on. So maybe, for example, TV watch hours, if I am computing,
[10:32:09] rather YouTube watch hours seasonality should be telling me that yes on weekends people watch more rather than weekdays or at the night time maybe people watch evening time people maybe watch more compared to the daytime because daytime people are in offices and so on. So there could be seasonality of different kinds and if there is seasonality you must explicitly encode it and therefore people keep
[10:32:29] up with this thing called as triple exponential smoothing. So the idea is that I'll not go into deep details about how these three, I mean, how is this entire thing going to work, but I'll at least give you some intuitions. So if the data contains seasonality, double exponential smoothing does not capture that seasonality.
[10:32:51] And therefore you need to include a third thing so as to basically do seasonal smoothing also. So in the double exponential smoothing you had this overall smoothing, in the double exponential smoothing you had overall smoothing, you had trend smoothing. Now you also need a seasonal smoothing. And that is why you have three parameters now and therefore it's called
[10:33:13] That's triple exponential smoothing. So it is also called as the Holt-Winters method. So the previous one, remember, is called as the Holt-2 parameter method. So this one, double exponential smoothing is called Holt-2 parameter method. This one is called as Holt-Winters method. And it has these three parameters, alpha, beta and gamma.
[10:33:35] As you can see again, this ST is going to be very similar to what you observed on the previous slide, except for this IT minus L. If you just remove that guy, everything else is just the same as you saw on the previous slide. And then what you would observe is that your trend smoothing does not change. It's the same thing, but there is something called
[10:33:58] seasonal smoothing? What is seasonal smoothing? So, you know, just like trend smoothing tries to estimate how your trends would vary, how your hikes would vary, right? Previous to previous year, I got like 5 lakh hike. This year, I got 6 lakh hike. Next year, I will get 7 lakh hike. That's the trend smoothing part, okay? Seasonal smoothing basically thinks about
[10:34:20] things like hey previous Diwali I sold 500 lamps, previous to previous Diwali I sold 400 lamps, now will I sell 600 lamps is the question and that's the kind of thing you know this Diwali so basically you know this is seasonal smoothing so in the sense that it tries to estimate how much the series will vary at a gap of seasons at a gap of
[10:34:45] Vegas season. So you see IT is being expressed in terms of IT minus L. Remember not IT minus 1. Okay. Unlike the overall smoothing and trend smoothing, this guy depends on IT minus L. What is L? L is the season. Okay. L is the size of the season. So L is one year, L is one month or L is one week or things of that kind. That is what is
[10:35:06] L basically the size of the season the period basically. So that's that so we will again not go into deep details of the formulae but you more or less get that that yes there have to be three parameters to capture the kinds of things the inherent nature of the time series itself overall smoothing the trend and the season seasonal effect and that basically helps me get the forecast.
[10:35:24] Forecast over a period time point is going to be like that. ST plus MBT multiplied by the seasonal value this time because now you have this seasonality also playing a role. And also you observe that the seasonality is captured in a multiplicative way. So seasonality is
[10:35:40] captured in a multiplicative way. That is why when you are trying to forecast you multiply. So that was about exponential smoothing. So remember single exponential smoothing, double and triple exponential smoothing.
[10:35:58] The parameters that you see are not literally learned. They are basically just tuned. So there is no learnable parameters. Again, this is the case because these models are old. They are basically a few decades old in time and they are very important still because even today,
[10:36:22] you know you get time series where you do not have a very long time series and if you don't have a long time series you can't use regression you can't use deep learning and so on right so you would still need to use these kinds of models so they are still very prevalent now compared to these models the next
[10:36:44] The next interesting models are called as the ARIMA models. The next set of things is called ARIMA models. So just to basically give you the big picture, we start off with simplest models, naive forecast, you know, hike-based models. Then we talked about average, full average, and then moving average, right? But then we looked at these exponential
[10:37:10] kind of models. And then I'm going to talk about these ARIMA kind of models now, okay? So in exponential smoothing, we looked at single, double, triple ARIMA. Basically, we'll just look at the standard ARIMA model given the time that we have. But there is a whole bunch of series of models. For example, there is a ARIMAX model, there is a S-ARIMA model, there is a V-ARIMA model,
[10:37:38] So there are these variants of ARIMA also. And after that, people also came up with other kinds of models like ArchGuard models and so on. So I'm not going to get into those, but we will talk about RNNs right after that, because those are the ones that are popular if you have longer or other larger number of time series data to train your model in that sense. So let's talk about ARIMA. So ARIMA model.
[10:38:10] basically follows a process, which is what is visualized on this. And by the way, are there any questions before that? Maybe I can take those questions. If there are questions, let's see. Vinod Kumar has a question. Leveraging this forecasting models, can we predict next political party is going to rule or Indian economy stands at top two? What are the probability system? Yeah, absolutely. We know, right? Yeah, I mean, you know, we can always use these kinds of models to do all kinds of forecasting and
[10:38:43] and hopefully for forecasts turn out to be accurate, right? We become as popular as Baba Vanga, right? Thank you. Yeah, thank you. Yeah, thank you for bringing in the fun part. Yes. I mean, see, I mean, after all, all of these models are machine learning based models or even not even machine learning. Some of these that I've talked about are just heuristics folks. So yes, I mean, in absence of anything else, I mean, the idea is that everybody is
[10:39:18] is about the future, clearly, right? In absence of anything else, these work better, but doesn't mean that they'll give you like 100% accuracy, right? So that's basically that. So that's where I would leave that thread. Yeah. So the next one is ARIMA time series models. The idea is to follow this five-stage recipe, visualize the time series, stationize the time series, plot something called as ACF-PACF charts and find optimal parameters. Fourth one is building the model actually and fifth one is making predictions.
[10:39:57] So in the traditional way of doing machine learning, building the ARIMA model is literally training the model. So this part is basically, you can think about training the model, training the model. And making predictions is literally like testing, or essentially inferring. So inference or test, as you can call it, inference. So inference or test. So what are these things? These things, you can really think about them as pre-processing. So you can think about them as pre-processing steps. I mean, in the modern machine learning paradigm, we can think about these as the steps. So first step is visualize a time series.
[10:40:34] Now, how complicated could visualization be? And what should you do when you visualize? You visualize so that you can stationize. So you basically figure out if there is a need for doing this pre-processing or not. You visualize. So what does stationization mean? So basically, the output of stationization should be a stationary time series. So as I said earlier on the previous slides, making a time series stationary is a pre-processing step before you can run Anima model or train Anima model on it. So therefore, we must
[10:41:19] understand what is a stationary time series first. So a stationary time series, I mean, a whole bunch of blah blah is written here, which you can read. It's all mathematical, but I'll explain it in simple words. So on the right side, what you see is a simple time series, and that helps you understand stuff. So the green ones are stationary time series, the red ones are not stationary time series. Yeah, it's also written right below the plots. So in a stationary time series, various kinds of properties of time series are basically not a function of time. That's the simple thing to observe.
[10:42:08] So for example, if I look at the mean value for any window, so basically, if I take a window, if I take this window, or if I take that window, the mean value is different. So if the mean value differs, or in simple words, if there's a trend, then that is not a stationary time series. So basically, a stationary time series is like, if I take a window, if I take compute the mean in that window, more or less the mean is going to be similar. in a stationary time series. So in a non-stationary time series, mean changes over time, but in a stationary time series, it is more or less stable with respect to time. Similarly, if I talk about variance, so you see, I mean, here the variance is so high
[10:43:02] here the variance is so little. So you see variance should not be a function of time in a stationary time series. So in a stationary time series, you know the variance is constant over time, okay, in that sense. This property is also given an interesting name. So, you know, if the variance is constant is not a function of our time. It's not a function of time. It's called homoscedastic time series, homoscedastic time series. I mean, again, these jargon may be interesting to remember for interviews. But for all practical purposes, you should really think if the mean variance or something called as autocorrelation changes over time, then it is not a stationary time series. You must do something to make it stationary. So what is autocorrelation?
[10:43:55] simple words, it's like this. If you take any window here versus the same sized window there, what you would observe is that, you know, in one of them it is compressed, in the other one it is not compressed, right? It's basically pretty relaxed in that sense. And that's basically, I'll talk about this autocorrelation guy more in detail in the next few slides, but that's a simple way of understanding. Well, I mean, you know, this doesn't look like a uniformly spread out type series. And therefore the red ones are all non-stationary, the green ones are stationary. So that's what you can observe either via visualization or by running some simple Python code to figure out or detect if the time series is stationary or not. If basically mean, variance, autocorrelation, do they change over time or not? If they change over time in the sense that if it is not stationary, then the step is to actually make it stationary. So then what you need to do is to
[10:44:38] basically make it stationary. So essentially, how do you make time series stationary is the next question. But before that, why care about stationarity of time series? Because it's a pre-processing step, mandatory pre-processing step before you can do ARIMA. So until unless your time series is stationary, you cannot build a good ARIMA model. And when it is not stationary, what you need to do is to stationarize it. Now, how do you stationarize a time series is the question. So there are multiple methods of doing stationization. So two of them are this detrending and differencing. I'll just explain it via Excel right away, very quickly. How do you take a time series which is not stationary and you make it stationary? So let me create a time series which is not stationary. So basically, this is a time series which is
[10:45:11] which is not stationary. Let me basically just plot it nicely so that we can appreciate the dynamics. Yeah, so this is some time series. Let me just plot it and show to you that it is not stationary. So it is, as you can see, you know, it basically has an upward movement. So it has trend in it, right? What I want to do is to make it stationary. How do I make it stationary? Okay. So to make it stationary, it is super simple. What I do is to basically take the differences. So I take the second value.
[10:45:43] I subtract the previous value from it. So, I just take the differences. So, how do I remove the trend in a time series? I remove the trend by just taking the differences. Now, if I basically insert a chart over this new time series, you would observe that the trend is gone. The trend is all gone. It is more or less flat now. It is more or less flat now. So, defencing is a nice way of removing the trend. So, taking a difference is a nice way of removing the trend. Another way of removing trend is as follows.
[10:46:15] Basically, take the time series, you try to fit a curve. So how do I fit a curve? I add a trend line to it. So maybe a linear trend line. And maybe I can also say display the equation. So that's the equation and that's my trend line. So it's a simple equation, very, very simple equation. And you see, I mean, again, let me also put the notion of time here. So this is my original time series. This is my differenced time series. And I can actually plot my differenced time.
[10:46:46] maybe I can plot all of them together at some point. Yeah, when I am done. And this one is called as de-trended time series. How do I compute the de-trended time series? Well, I take the original time series the fitted values subtract the fitted values. How do I do that? So I take the original and subtract the fitted values. Now, I mean, I don't know how do I compute the fitted values. So fitted value is given by this formula. So I write the same formula 1.9057 multiplied
[10:47:14] It says x. What is x? x is my time point. I put my time point plus 0.4133. You see where I am getting that equation from. I am just getting it from there. Maybe I realized that I should have increased the font size so that you guys could see it nicely. Hopefully you can see it nicely now. So that's that. So I basically just use that formula to compute this detrended value, the original value minus the trend.
[10:47:39] and that I do for every every time point and now what I have I mean this one if I don't have a value I can just fix it to the original value. So what I can show you is that if I try to insert you know these guys nicely I hope this will be okay yeah you see the original time series basically was like that and you know my old plot is on the right side so the time series with the fitted curve but both of these the orange and the and the and the
[10:48:03] ones are basically the time series without the trend. This is called as differencing or detrending, which is basically what is mentioned in the slide. How do I remove the varying mean over time kind of property? I need to remove trend and the way I do that is by literally fitting a line and detrending, removing that trend or just by taking differences. Either of them are a good way of
[10:48:23] of making the time series stationary. So we already talked about this stuff, which is basically differencing. We also talked about detrending. The next thing is non-constant variance. So you see in a non-stationary time series, this variance could change over time, which we don't want. We want it to have more or less uniform variance.
[10:48:45] Now unfortunately, you can't do preprocessing to suddenly remove the variance. But what you can do is to reduce the impact of variance. So you can reduce the impact of variance. And the way you do that is by taking the log or the square root of the time series. What that does is to reduce, you see, I mean, when you take log, extreme values,
[10:49:05] become smaller similarly square root extreme values become smaller so you cannot remove the non-constant variance part but you can reduce the impact of non-constant variance by making it by making the variance not fluctuate too much by taking log or square root so that's another way of doing of trying to make the series as stationary as you can
[10:49:26] So, and then the last thing is basically trying to also take care of seasonality. So, sometimes your time series could also have seasonality and you want to remove seasonality. To remove seasonality, what you could do is to do the same differencing operation, but with respect to the previous season.
[10:49:45] example, if the seasonality is over one week, you take the current value and subtract the one week past value from it. So basically saying there are all of these interesting pre-processing operations, detrending, differencing, taking the log, taking the square root, doing a seasonal difference, all of these operations
[10:50:04] are basically done to stationerize the time series. Once the time series is stationary, you are done with these two steps of your ARIMA analysis. There is still one other preprocessing step left called as plotting this ACF and PCF charts and we will understand what those are and then you would build the model and make
[10:50:24] So we'll talk about those things, but I'll pause for questions if you have questions at this point. Let's see, there are two questions. Why the third image in red isn't stationary? Oh, we talked about that. I assume you understood that part, autocorrelation structure. The autocorrelation structure is varying over time.
[10:50:47] in this third one. And unfortunately, there is no nice way of repairing that. I mean, there are some complicated ways, but rather than that, you would rather just work with whatever you have. So if the time series has two extreme levels of autocorrelation, you just understand that your Karima analysis may not give you the best models. But that's the answer.
[10:51:09] to your question, Vasanth, right? Taran is asking, stationize then doesn't work for when we have trends, since the mean will also trend. I think I asked this question too early, so you can... Oh, okay, so you understood now, right? Okay, awesome, awesome. Thank you. Thank you. I mean, I was trying to understand, but I think you got the
[10:51:32] anyway. So that's good. Thank you. Taran is also asking, detrending and differencing works well with the linear sample, but how would work for, say, a stock price that goes up and down over a period of, say, two months, but for a period of five years, it's an upward trend. Good point. Yeah, I mean, see, it basically tries to address the trend, Taran. So the point
[10:52:07] is that if it goes up and down, then you would not call it a trend anyway. So it tries to address the global trend. Also, I must mention at this point, folks, I mean, when people were doing this Arima kind of stuff, they were not really thinking about time series, which have like 1000 time points or things like that. They would typically work with much smaller time series and few tens of time points at the max.
[10:52:47] So part of the question is answered in the sense that if it's a short time series, you would want to really capture about global trend. You want to talk about the global trend. If there's no global trend, then these methods, then that's not called a non-stationary time series. It's just that, yeah, there is variation and you can't remove it by doing trend analysis in that sense. Okay. Okay, good points. Let's keep going. So basically figured out how to station as a time series. Now the next step to do is to essentially plot
[10:53:27] ACF and PACF, but first you need to understand what those ACF and PACF mean. Sorry folks, discard. And yeah, I was trying to make sure that I hide these floating zoom controls. Okay. Yeah. So basically, yeah. I mean, what is ACF and what is PACF? Let's try to understand that part. So we did this. Yeah. So let's talk about ACF first. Okay. ACF stands for auto correlation function. And let's try to understand what is autocorrelation guy means. So let's consider a simple time series of this kind. So the tool bar and yeah.
[10:53:55] So let's consider a simple time series of this kind. All of us have seen this time series at some point. It's a nice sine curve. So it's time. Now you see what I can do is to plot a time series which is shifted by one time unit.
[10:54:21] unit. So that's basically called as lag. So lag equal to 1. That's one kind of time series. Now you know what I do is to plot yet another kind of a purple time series where lag is in a very very interesting manner. This is the lag. So you see the kind of a lag. So this time series is the ultra of the previous time series. You see red and purple have opposite trend in their senses. So if red
[10:54:45] goes down, I mean not the word trend in the sense of upward or downward trend, but the whole point is that they are behaving in an opposite manner. So and there is a particular value of lag, maybe this lag is lag equal to 4 for that matter for this time series. So whatever is the lag basically, if I really compute the correlation between the red and the purple one, what do you think will be the correlation? It will be minus 1.
[10:55:06] they are behaving in opposite manner in that senses and you see if I basically have a some interesting lag for example if the lag exactly comes to this point you know if I basically delay or shift the time series to the right exactly by that you know that is how my lag is that is how my time series is going to look like okay so the yellow and the red actually overlap which basically means the correlation
[10:55:25] is going to be 1 at this value of lag whatever this value is maybe lag equal to 8 that's the value. So you see when the lag is equal to period correlation value between the original time series and the new time series is 1. Does this make sense because they overlap.
[10:55:43] when lag equal to period or two times period or three times period or four times period, my correlation value between the original time series and the lagged time series is one. So now coming back to this one, what is autocorrelation? You see autocorrelation
[10:56:01] Correlation is basically computing correlation between a time series and a lagged version of it. So what is autocorrelation? It is the correlation of a signal or a time series with a delayed or a lagged copy of itself.
[10:56:22] So that's what is called as autocorrelation. Now this formula may look very complicated but this is something that you have seen already. So this is nothing new. So remember you computed Pearson correlation and you know the formula for Pearson correlation. What is it? If you have two vectors x and y, this is the formula for
[10:56:38] correlation. Remember, you take the deviation from the mean and see how they covary divided by the standard deviation of X and Y. So, covariance normalized by standard deviations of X and Y gives you correlation. When you do
[10:56:57] have x and y two different vectors, but you just have y and y t minus k. So you have like the original time series and the lagged value of time series. It's called autocorrelation, correlation with the lagged copy or a delayed copy of itself, therefore called auto.
[10:57:16] So as I mentioned, if you basically vary the lag and you plot these autocorrelation values, which can go from between 1 and minus 1, it's called the autocorrelation plot or autocorrelation function ACF.
[10:57:35] If the time series is periodic, then the ACF would basically look like that. I mean, you know, there will be a spike of one. There will be a spike of one at periodic time intervals. So basically, this thing needs to be equal to period. So the gap between the two spikes will be period. Why?
[10:57:54] I really showed you using this example. So look at the golden time series and the red time series. Well, if you compute the correlation between them, you would basically see the correlation is one because they overlap. So ACF function is a nice function to compute the
[10:58:14] in your time series. It tells you how periodic the time series is because if the time series is periodic, you know, you would see these spikes in the autocorrelation value. Okay, you will see the spikes in the autocorrelation value. Basically, they will be and they will be separated. These spikes will be separated by the
[10:58:33] that period. So that is why ACF is useful. Now besides ACF, there is this thing called as partial autocorrelation function also. And that function is also drawn like this. It's the same thing on the x-axis you will have lags, on the y-axis you will have partial autocorrelation value.
[10:58:55] Now, these partial autocorrelation values are slightly more complicated mathematically. In fact, they are actually obtained by performing a regression, in fact. Intuitively, this is what it means, partial autocorrelation at a lag. I mean, obviously it is defined at a particular value of lag. It is the autocorrelation between x t and x
[10:59:16] minus K that is not accounted by lags previously. So the idea is that if you basically take the previous lags, you use them as features to predict what would be the lag at the next time point, you know, whatever is the residual is called as partial autocorrelation. Yeah, so it's mathematically slightly complicated to understand. So if you don't understand
[10:59:37] understand it's fine. You basically just take away this thing that hey PSEF is another function like ACF anyway. So ACF hopefully you understood. ACF is just autocorrelation between the time series and delayed version of itself. PSEF is something similar. But PSEF has to be drawn after you've drawn the ACF because
[10:59:55] because it depends on ACF. We know how to plot them, but we still don't know why did we plot them. So we plotted ACF to just figure out whether there is seasonality or not. And if there is seasonality, you must remove seasonality by
[11:00:16] by doing seasonal differencing. So that is why you plotted ACF. Why did you plot PACF? It's unknown so far. And I'll talk about why you plotted PACF in the next step itself. So I'll quickly talk about why you plot PACF.
[11:00:38] Okay, before that, I'll take questions. So Rajnikanth is asking, let's take an example for a system where CPU memory and disk metrics are taken, right? These three are different metrics and values are different in nature. How can we choose a model to fit for all type of metrics? Good. I mean, yeah, I think this is a great question. Yeah.
[11:01:01] And I assume you want to predict maybe you know the CPU usage in the next time point or the memory usage in the next time point. So Rajnikanth is talking about a system which has multiple time series interacting with each other. They do depend on each other. I mean, you know, or they could be independent also. Okay. Another example of such a system is the stock market itself, right?
[11:01:22] Stock price of Microsoft depends on the stock price of OpenAI and so does Oracle right now. I mean very very tied future in that senses of these companies. If one does well, the other two will do great. Anyway, so basically saying yes time series could be dependent on each other. So far all the models that I have talked about do not incorporate
[11:01:45] it is interdependence between the values of time series over various time points. But there are interesting models which can help you capture that. So for example in the ARIMA world itself there is something called as V-ARIMA or Vector ARIMA. So what we have focused so far is ARIMA and I have not even talked about you know how does the prediction happen in ARIMA.
[11:02:09] But in ARIMA, it basically just works on univariate time series as it is called. So you have only one variable that you are measuring for. But what you are asking for is multivariate time series analysis. And there are models, as I said, vector ARIMA helps you do that. We will not do that in this session, even the time. Another way of doing vector, you know, or rather a more advanced way of doing
[11:02:33] this multivariate time series analysis is using RNNs, RNNs, LSTMs, those kinds of models, which is what I'll touch upon anyway. So hold on for some time, but what you want is basically called as multivariate time series analysis, which is different from the univariate time series analysis that we have been talking about so far. Okay, thank you. Awesome. Okay, let's keep going, folks.
[11:02:58] So let me basically quickly talk about, you know, why did we plot ACF-PACF? And then let's see how this is useful to build the ARIMA model. Okay? Yeah. So that's that. This is a complicated looking slide, but I am with you. Don't worry. It's not complicated. And it's something that you, partially you already know about it and the remaining part you'll understand.
[11:03:21] anyway. So it's not very complicated, although it looks like that. So basically, you see ARIMA actually is a three stage, it's three different parts. AR stands for autoregression. So as somebody asked, yes, it actually does autoregression. Somebody asked before, hey, it's just linear regression. Sure, I mean, AR part actually does that. MA part is called moving average. Now,
[11:03:47] There's nothing moving average about it. Remember, it's a misnomer. It's just historical. So it's not a moving average, but it's called moving average. Remember that part. The third thing is actually i. The i is also called as integrated. So i is called integrated. So we'll talk about all of these three parts one by one. And all of these three parts,
[11:04:10] actually have a parameter so p d and q so the idea is that there are three parts a r i and m a these three parts come with three parameters p d and q okay and these three parameters specifically the two parameters p and q are estimated using the acf and pscf plots that we drew okay so these acf and pscf charts that we drew these are used
[11:04:34] in estimating these P and this Q and that is why we drew them. So why did we draw a safe and PSEF? Because we want to estimate P and Q. Why are P and Q required? P and Q are required for learning the AR and MA part of the ARIMA model. That's the flow. Obviously we need to understand what is AR and what is MA. So autoregression is basically
[11:04:57] what you can simply understand. It's a very simple thing. So the idea is that if you want to estimate the next salary or the current, let's call it the current salary, based on previous time point, what you could do is to basically, in the simplest form, you can just try to give some weightage to your previous year salary plus some bias and that's your current salary. Or what you could
[11:05:17] do is to create a model with like 10 features and you can learn those alphas. So what you could do is to basically compute your current salary as a weighted combination of your past few salaries, P past few salaries with some bias. So this part is nothing different from linear regression. It's basically linear regression where you are using the previous
[11:05:35] time point values as features where you are using the previous time point values as features. Hopefully this makes sense. So the idea is that you actually compute the current value as a linear weighted combination of your previous values where the weights need to be learned, the weight
[11:05:53] need to be learned just like you would learn in a linear regression along with a bias epsilon t. So that's the AR part. AR part is super simple to understand. Now the idea is that sometimes sudden shocks happen in the time series. For example, a life you
[11:06:11] happens. And that life event is not like it will have an impact only on your current time point. For example, let's say you have a baby and if you have a baby and typically this is very much valid for females. So essentially if you just got a
[11:06:31] baby. Now, obviously your salary may take a hit on in the current year, but it will also take some hits in the next few years. So in short, you know, if there's sudden shocks in the time series, which can happen, right? They also propagate over years. So let's call those sudden
[11:06:53] as epsilon t's. You see these sudden shocks are not visible but these sudden shocks are changes in your time series which happen because of some events which are not encoded in the values of time series. So let's call those sudden shocks as epsilons. Those epsilons are happening over time and by the way this is not the same as that
[11:07:16] the AR guy. Okay, so please don't mix these two. They are not the same. Okay, so you see what I'm trying to talk about is that some sudden shock, sudden events keep happening year over year. So they will have some effect on your next few salaries also. So now how much effect did this sudden shock have on your next few salary is determined by this guy called beta.
[11:07:39] So you should really think about this guy also as a linear regression. It's just that you do not have the feature values also. So it's actually a little fancy. The idea is that you are trying to first estimate the latent sudden shock that happened. Latent means hidden. It is not observable. So you don't know how much shock happened, but you know that some shock happened. Something happened. So in the
[11:08:14] And that particular event could have an impact on the next few time points also and those impacts are captured by these betas. Now, I mean, you can always argue that certain kinds of shocks will have a different kind of an impact in the future and so on. But this model is simplistic in the sense that it assumes that whenever a certain shock happens, all of those shocks will have similar kind of an impact on the future.
[11:08:49] which is captured by these thing called as betas. And this particular way of modeling it is called as the moving average model. Now it's not literally moving average as you can see, it's a weighted moving average. That is why I said moving average is a misnomer. It's a weighted moving average. I mean, you know, weighted moving average with the window size being Q. So in autoregression, it is basically making use of previous values as features and the window size is p in the sense that you care about past p features.
[11:09:26] average model. It's literally a weighted moving average. The right way to call it is weighted moving average, where the weights as well as these certain shocks have to be estimated. So here the variables, overall number of variables, there are quite some. There is these alphas, there are these betas, and there are these epsilons. All of them need to be estimated accurately. So the overall model is written like that. It's written like that. There is this, you know, AR part. and there is this moving average part, the moving average part. There is a third part also.
[11:10:08] I which I talked about or rather I did not talk about which I am going to talk about now. But hopefully the AR and the MA part should be clear. The AR and MA part are dictated by these parameters called P and Q and they are actually fixed using the ACF and the PSCF plots. Now two open questions are there. One, what is this D guy and I guy? And second, how are these P and Q set using ACF and the PACF plots? I will talk about both of them one by one. So basically, we do not know how are these P and Q
[11:10:41] set using ACF and PACF plots. And second, we do not know what is this i guy. So let me talk about the i guy that's easier. So the i thing is much easier and let me just quickly talk about the i guy. You know, let me actually create a new time series and the time series is as follows. There is this thing that I'll start with, you know, one and maybe the next number is 2.1. The next one is, you know,
[11:11:08] Okay, this is good enough. There's a time series. And I'll basically just show this to you. The time series has this trend. So I need to, this is the original time series. So I need to detrend it and maybe I can just put the time number.
[11:11:34] also here so that you have time. So if I do detrending, what would I do? I'll probably fix this as one as it is, but this one should be this minus this. This is differencing, right? Differencing is one way of detrending. Sure, you do that. So let me basically just delete this guy and try to plot this, you know, difference time series. So I try to plot it, and if I plot it,
[11:12:00] this is how it looks like. Maybe this is a bad way of plotting. Sorry. Maybe this is I just plot these two. Yeah, I think this is good. So the original one is basically the blue one. The different one is this orange one. But that by itself also is a trend. I mean, in fact, if I basically just fit a trend line to it, I would observe that, well, there is a trend.
[11:12:23] here in the orange one also, so in the orange one also if I basically show you the trend line and show you the equation, display the equation, it has a nice trend, it has a positive trend of 1.02. So I mean if I want to remove that trend also what I could do is to do it two levels, so level two differencing, which
[11:12:47] basically means I take this minus that. And that's basically my level two differencing. And now let me just plot these guys. In fact, let me try. But well, the magnitudes are different, so it won't look very nice. Let me just plot these two and show it to you. So you see, this one has an upward trend, but this one does not have a trend.
[11:13:11] I mean one can always say oh there's an upward trend and then a downward trend but that's not that's not the point I mean you know the a rather better way of saying is that clearly this blue one has a higher trend compared to the orange one okay in fact if I fit a trend line I mean if I try to just add trend line you know this more or less has a very small slope so let's see if I display the equation it's like
[11:13:35] 0.14. It's much much smaller slope. So the idea is that folks please use the Q&A box if you can. Otherwise I'll take your question but after some time. So the point is that rather than doing one level of differencing, two level of differencing helped you do this stuff. Helped you figure out a lower amount of trend and that is why sometimes people do two level of differencing and the number of levels of differencing
[11:13:57] is dictated by this D, so D equal to 1 or 2 basically is decided by how you can reduce the trend overall, how you can reduce the trend overall. So I stand for two things, one is integrated in a sense just merging AR and MA, but I also stand for how many you know this differencing did you do so as to detrend your time series, so that's basically
[11:14:20] the I guy. Now the only thing that is left is how do you decide P and Q and let's just quickly talk about how do you decide P and Q and then I'll pause for questions. How do you decide P and Q and then I'll pause for questions. So I already talked about what is AR, what is MA, we talked about I, we also talked about you know, yeah I mean you know
[11:14:38] the MA model, which is basically linear regression on random shocks and their impact as propagated in the next few time points in your time series. So you see this P and Q is decided by the shape of the autocorrelation functions, the autocorrelation ACF or PACF.
[11:14:58] So you see this is like a rule book that you see on this slide. So if the shape of the ACF, you know, the shape of the ACF autocorrelation plot is exponential and decaying to zero, okay, exponential and decaying to zero. I'll show you an example on the next slide actually. Then you basically,
[11:15:20] use only the autoregressive model. You do not use the MA model at all. In fact, you set Q equal to zero. That is what this means. But what should be your P, the order of the autoregressive model? Well, that's decided by the autocorrelation plot. Use the partial autocorrelation plot. Use the PSEF to identify the order
[11:15:43] of the autoregressive model. Yeah, so that's that. So and then there are rules for each of those shapes. Remember folks, I mean these are old models. I mean they depend on a whole bunch of heuristics. So these are the heuristics that are there in ARIMA and you have to basically depending on the shape try to figure out the order of the model that you will try to fit.
[11:16:04] So the idea is that if the shape is exponential and decaying to zero, what would that mean? Exponential and decaying to zero. So, you know, exponential and decaying to zero means, yeah, I mean, essentially just decaying to zero like that. That's exponential and decaying to zero. Alternating and positive and negative decaying to zero. That would mean this.
[11:16:24] If this is zero, this is zero, then alternating but decaying to zero. That is what the second thing would mean. And so on. So depending on the plot cash shape, you would use what is written on the right side. So you would build an autoregressive model or a partial autocorrelation or a complex model.
[11:16:43] model and you would basically decide the orders based on the partial autocorrelation plot. So let's look at an example and that's basically that. I'll basically move to the ML viewpoint after the break. So basically I'll look at the example, take your questions and then we'll take a break. So here is an example.
[11:17:03] This is an example of an end-to-end ARIMA kind of a model. So you see this is some original time series. The original time series when plotted and visualized is also called as the run sequence plot. So this is called as the run sequence plot. This is something I have learned here about with some carbon dioxide concentration or whatever over time.
[11:17:22] So you know you see in this time series you basically detrend it so detrend you will basically do differencing or remove the trend using a linear fit whatever way and you basically get this kind of our time series. What you observe is that maybe you did not visualize it right you were just doing it
[11:17:52] So, you don't visualize it and you don't realize that there is a seasonality. Therefore, what you do is basically just go ahead and start plotting your ACF plot. You create your ACF plot and this is the ACF plot on the right side. This is the autocorrelation plot. Now, once you see the autocorrelation plot, you observe
[11:18:24] are essentially at equal time intervals. You see, there are these spikes, you observe the tips, the tips basically are at equal time intervals. And if ignore the red and the violet or the blue curves for now, just look at the black lines, right, gray lines, so to say. You see that they are spiking and these spikes are of the same size, right? It's not like they're decaying or whatever, right? So this basically just tells,
[11:18:58] that there's periodicity in this curve, seasonality in this curve, okay? And the gap between these two spikes tells you what is the season. The season is 12 months, yeah? The season is 12. So once you know that, you basically do de-seasonalization, basically meaning, you know, seasonal differencing. So you take the difference of the current time point, 12 time points back in time, you take that value and you subtract it, okay? That gives you de-seasonalized time series. Now there is a de-seasonalized time series on which you plot an autocorrelation plot.
[11:19:30] Okay, and when you do the de-seasonalization plot or autocorrelation plot because the seasonality has been removed, you will no longer see your spikes. What you see is this second type of curve. Remember this one? alternating positive and negative decaying to zero because you know alternating positive and negative decaying to zero I mean decay is happening right and you know this this blue thing this blue line and the red line actually tell you the 95 percent and 99 percent confidence bounds in the sense that if it is within those blue I mean you know if it is within those blue let me plot
[11:20:04] If it is within these blue, it basically means that it is almost close to zero, okay? With a 99% confidence in that census, or 95% confidence, sorry, yeah? So therefore, the point is that now you have realized that it is of this shape, so therefore you draw a partial autocorrelation plot, and then in the partial autocorrelation plot, let me just quickly zoom in so that you can see this partial autocorrelation plot nicely. Wait, did the zoom thing go? No, no, no, no, sorry. Sorry, I...
[11:20:33] Made a mistake. Yeah, maybe I can just zoom in like this. It should just have been here. But anyway, yeah, probably you can still see this thing. Let's discard this first. Yeah, so what you see here is that in the PSEF plot at the second time point, this is time point 0, time point 1, time point 2, time point 2 the value is close to 0, very close to 0. So that is why you basically will use P equal to 2.
[11:21:04] The p-value is going to be set to two. I mean, as it says, you're going to make an autoregressive model. Yeah, I think I can zoom it now. So let me go to the next slide. In fact, ouch. Go to the next slide and then zoom in. So what you see in this PSCF plot is that at time point equal to two, your value is almost zero. So wherever it crosses zero, that is the value, that is the time, or that is the lag that tells you that should be the order of your model, okay?
[11:21:37] Since it crosses 0 at 2, you basically say you are going to make an autoregressive model with parameter p set to 2. So this is how you basically use ACF and PCF values to basically estimate the PDNQ. Once you have estimated PDNQ, you create the model and this part is important folks. You create the model, you train the model and how you train the model? Well, I mean, you know how to train linear regression kind of models and this is more or less like linear regression with several weights. You train the model, you train alphas and betas and epsilons.
[11:22:10] At the end of it when you have the model you are going to apply the model and essentially predict the next value. But remember your next value still needs to be seasonalized and trended in the sense that remember if you did de-seasonalizing in the sense that you subtracted the seasonal differences, you have to add them back. And you basically need to also add your trend back. So for example, if I look at it right in my example on the Excel sheet, I basically subtracted the previous value. So in this level two
[11:22:39] time series if I do all of my forecast and let's say my forecast comes out to be something I know 1.2 is my forecast based on these values. So if the forecast is 1.2 I must add back the trend that I subtracted so remember here I subtracted some stuff I subtracted F29 so my forecasted value should be this plus my F29 which was basically this guy. and it should be this plus you know this guy because I subtracted that also so you see a rather sorry my bad I have to
[11:23:16] I have to add the last one. I have to add the last one. So, you know, this guy and I have to add the last one. So I have to add the last one. So whatever I subtracted, I have to add that back because, you know, whatever pre-processing operation I did before I did the ARIMA analysis, I must basically do the reverse of that pre-processing operations. If I took a square root, I must square it. If I took a log, I should do anti-log. Yeah. So whatever I did, that operation has to be reversed after the forecast has been done by ARIMA model. Okay. And that's it folks.
[11:23:56] I mean, I think that is what Arima is all about. Now I will pause for questions and then we'll take a break and then we'll talk about RNNs after the break. Okay. If you have more questions, please feel free to put them up. Okay. Rajnikanth has a question. Let's take an example. Oh, I think that one I answered already. Noor Fatima has a question. Could you please take an example of what we are removing exactly when we are removing trend and what remains after removing trend? Yeah, I think it became clearer as I gave more examples Noor Fatima, right? So essentially when we are removing trend, we are basically just removing some part of those values, some, you know,
[11:24:30] magnitudes and yeah I can't see who has raised their hand but somebody has raised their hand also. I mean you know by asking this question you know that you are being bad to these three people who have typed their question right. Why don't you just type your question, Ram, if you don't mind, right? Just being nice to everyone, right? Please, yeah? Or else, let me take your question after I'm done with this. But first, let me answer these questions, okay, if you don't mind. Yeah, thank you. So, you see, so, you know, when you remove the trend, what you remove are, are these
[11:25:07] going values. So you reduce those magnitudes, right? And they are just differencing. I mean, in a sense that you can reverse this operation. If you know the trend, you can always add back the trend, right? After your forecast is done. So in some ways, if you think about ARIMA analysis, it's sort of taking your time series as dividing it into the trend part, the seasonal part and a non-trend, non-seasonal randomly varying part, okay? So and the Arima actually works on this non-trend, non-seasonal randomly varying part. The trend part and seasonal part are simple to handle. So you remove them and then after you are done with the full forecast,
[11:25:42] you just add them back again. So that is what is going on in that census. Pinakin is asking, is D applied only to AR or is it applied to MA as well? Well, Pinakin, D is applied to the original time series in the sense that you basically do a differencing on the original time series, then you build the models. So therefore, to think about it, well, the D is applied on the original time series. Once you have the difference time series, then you apply the AR and MA model, both of them. Taran is asking, I did not get the estimate, I did not get estimate the life events in MA. Like how we estimate that?
[11:26:13] pandemic will happen or which side of bed will Trump wake up? Oh, good point, great examples. Yeah, you do not estimate the future events, Saran. Remember, you basically make use of the estimate of the impact that past events have had. Okay. So again, I mean, notice the words that I'm using. We do not estimate the events. We only estimate the effect that past events might have had. So the idea is to think about it as follows. If you basically got a salary of X lakhs, now you might
[11:26:44] have gotten a salary of X lakhs because you naturally grew in your, you know, in your way of understanding things and the way you work and so on. And therefore you got that salary this year, right? But what could have happened is let's say you have a baby, right? Or you had a baby in the past. Remember you had a baby. Okay. So it basically means that your salary dropped even though naturally if you made progress, right? in your maturity of thinking and so on so therefore your salary should have increased but since you had a baby
[11:27:18] work as much and therefore you know you salary drops. Now you had a baby is not what is quoted in the series but what is quoted is the drop. That drop is basically what is captured in these epsilons and nobody tells you how much drop happens so therefore you know it's not like oh you observe that there's a drop it's just that you try to expect that there's a drop and then that drop is not going to make a difference only at that time point and you know if you have a baby obviously have to care for the baby in the future as well so therefore it will have an impact on your next few times
[11:27:49] points also. And that impact of the event on the next few time points is beta. So a couple of things to understand Taran. One, I'm not talking about future events. Here I'm doing future forecast, but I'm not talking about future events. I'm only talking about past events and their impact on the future values. Second, I'm not even talking about discovery of past events. I'm only talking about the impact that those past events might have had on those time points as well as the time points after that particular event.
[11:28:17] So hopefully that answers your questions. That's that. Let me go back to the QA window. Sorry. Oh, yeah. So that's that. And now Ram, you had a question. Please go ahead. Yes. Can you go back to your Excel sheet once? Yes. Yes, please. Sir, when we are going for level two or level three or level four, definitely there is a The trend will keep on decreasing.
[11:28:41] exactly illustration we are taking out of this? Good point. Yeah, see the idea is that you will never have like a zero trend value. I mean, that's basically only possible in an ideal time series value, ideal time series. So you will continuously keep decreasing the trend. Now the point is that you could just stop after one detrending or you could basically stop after second level of detrending. So the idea is that if your trend
[11:29:08] value continues to decrease, you keep doing these Ds. I mean, you keep doing these multiple levels of differencing. For example, right, if I take this time series, which I drew and I probably have it here still. Yeah, maybe not. Yeah. So if I basically just try to plot this, let me just plot these right. And you would observe that if I do more levels of detrending, it's not going to really help too much now. Sorry.
[11:29:35] I have to move this guy somewhere. Yeah. So, yeah. So as you can observe in the chart below, I have gotten this green one. Further, if I do detraining and you do observe that level one helped a lot. Level two helped quite some. Level three, if I do, it's not going to matter too much. And therefore it's best to just stop at level two. Okay. No, no, that is correct. But how do you, I mean, is there a way to determine these
[11:30:00] For example, if it is a level two, I have to stop. What you could do is basically look at the slope. So essentially, let me just show it to you. So add trend line and add the equation as well. What you could do is to basically look at the equation. So this one basically shows you, you see the trend 3.93. The slope basically tells you the trend 3.93. The second one,
[11:30:22] you basically again at trend line you basically display the equation 1.02 the third one you basically again look at this sorry not for my time series but you basically look at this at trend line you display the equation sure i mean 0.12 okay now you try now you try to do the differencing again you will basically start observing that this will change this will probably become negative now
[11:30:43] And when that happens, you can stop. Make sense? So if it was already negative and it starts getting positive, you stop. If it was already positive and it starts becoming negative, you stop. Make sense? Sounds good? Okay. Great point. Because here the dataset is small, the dataset is huge, then you mean to say that we have to calculate the
[11:31:07] y value and then based on that we have to take a decision whether we have to stop it or not. Absolutely, absolutely and I mean it's not about the data set being small or not you basically just try to figure out this equation and I mean because you know not always you'll visualize things right. I mean most of the times you'll be doing it automatically without visualizing. In that case you basically just check when does the trend
[11:31:34] go on the other direction and that's when you stop. Make sense? Thank you. Awesome. Any other questions, folks? Are there more questions? Yeah. If not, there is some question here. Is it slope or y? Oh, I mean, you can't consider y. It's slope basically. Yeah. Thank you, Sudramaniam. Yeah. So, I mean, y is a variable. It's not a fixed value. You obviously consider the slope.
[11:31:54] basically tells you the trend in that sense. Okay, so folks, good time to take a break. Let's take a break and let's meet at 11.45 and we'll take it from there.
[11:32:04] you.
[11:32:38] Thank you.
[11:32:58] Ah.
[11:33:17] Okay.
[11:33:26] Thank you.
[11:33:35] Okay.
[11:33:44] Okay.
[11:33:53] . .
[11:34:02] You.
[11:34:37] Thanks.
[11:34:52] you.
[11:35:02] Thank you.
[11:35:32] I don't even care for my brother.
[11:35:42] Okay.
[11:35:53] And yes, a little bit faster.
[11:36:03] Okay.
[11:36:33] you
[11:38:10] . .
[11:39:10] Okay.
[11:39:19] Okay.
[11:42:04] . .
[11:42:12] Okay.
[11:42:26] Thank you.
[11:42:35] Thanks.
[11:44:27] Bye.
[11:44:52] Okay.
[11:46:31] Okay, folks.
[11:46:43] So let's resume and back. Yeah. Okay. Yeah.
[11:47:04] So far, I've been talking about the older statistical models. Let's talk about RNNs. And when we talk about RNNs, the idea is that we have to talk about from a machine learning standpoint.
[11:47:25] So in the machine learning, all of us know that we basically take the raw data, try to think about the machine learning model, a particular class of model that we are going to use to encode the data using a learning algorithm. And then we will use that model to come up with future predictions, okay?
[11:47:48] That's the ML viewpoint. So now if you really think about time series data, time series data is of this kind. There is raw data across time. So you have 1, 2, 3, 4, 5, 6, 7, and many other time points like this. And they are the samples. So these are the values, time series values in that census.
[11:48:11] From machine learning perspective, if I have to really train this to basically learn the weights to be given to previous time point values, what I would do is to take windows. So I will take the window of 1 to 4, 2 to 5, 3 to 6, 4 to 7 and rearrange the data in this form. So essentially, where the idea is that x1
[11:48:41] X2, X3 can be used to predict X4. X2, X3, X4 can be used to predict X5 and so on. So in that case, what happens is that these three values in the past, basically, or in the history, the latest three values in the history become my feature vector. And I'm going to use them to be able to predict X4 in that sense. So that's how machine learning regression model would work.
[11:49:11] So the time series prediction would be like this. You take these three values, predict the next one. You take the next three, predict the next one, and so on and so forth. So that's how you're going to be able to do the predictions. So now, I mean, in the slide, I basically talked about three. Why three? So there's nothing specific about three. Well, you would try using k past features, and whichever gives you the best results on your validation,
[11:49:37] data now is something that you can use. So you basically just tune that K and essentially make use of it. So obviously you can just do it using linear regression for that matter. So your function, however, could also be a neural network itself. So if you basically just make use of linear regression, sure, your F is just a linear regression, which is okay, which can give you some accuracy. But rather than F being just a linear regression, you could have
[11:50:04] complicated function there, more like an MLP, more like a multilayered perceptron. And what you would do is essentially as many samples as you have, you're going to really learn the features of this multilayered perceptron such that if it takes these three features as input, the weights have to be trained. The weights of the multilayered perceptron are going to be trained such that if you take the previous three values
[11:50:30] as input and these weights, the prediction should be as close to the actual value as possible. That's basically how you're going to use a multi-layered perceptron. So we all know how MLBs look like. You have some inputs, you have hidden layers, and you have output. Using these three passed values, you're going to predict some output, and hopefully this output is going to be as close to the actual value. So you can pass x1, x2, x3,
[11:51:03] try to predict x4, if it predicts great, if it doesn't predict, then you do a back propagation and upgrade the weights here. You do the same thing for the next sample, x2, x3, x4, and you try to predict x5, and you keep doing this, and you keep doing this until you have more and more samples. So the machine learning problem, the idea is to predict future samples, and we cast it as a regression problem.
[11:51:35] Sorry, model could just be linear regression or it could be a nonlinear model like MLP. How many past samples the future sample will depend on? Well, that can be tuned. That can be tuned using validation set. That can be tuned using validation set. In fact, people try to do this on various datasets. So like on a dataset of predicting prices of onions. And people found that if you're using MLP, the mean squared error is the lowest. But if you use, for example, one of the way
[11:52:10] One of the time series prediction models was the full mean, full average. Now that basically gave you the highest root mean squared error, but the MLP actually led to the lowest root mean squared error. And different models like ARIMA, ARIMA is here and so on, but MLP multi-layered perceptrons could reduce the error by a significant amount. This is a visualization. So this is on the left side is ARIMA, right side is MLP multilayer perceptron based predictions.
[11:52:42] The red curve is the actual curve and the blue one is the predicted one. The blue one is the forecasted values. So you see the forecasted values are very different from the red ones, but with an MLP multilayer perceptron, the forecasted values, the blue and the red curves are very similar to each other. So that basically just also shows another visualization to sort of show that a neural network based prediction is better than just doing ARIMA, which is basically just a linear model, just a linear model.
[11:53:15] But that takes me to RNNs, and RNNs can do much better at doing these forecasts. Specifically, they are very useful for one of the questions that somebody asked, hey, what if I want to do multi-series forecast, multivariate forecasting? So in RNNs, what you could do is to basically supply, if you have multiple series, multiple time series, right? So essentially, let's say your data looks like this, you have like really multiple time series. You have one time series, You have you have this
[11:53:52] one time series, you have another time series, you have third time series, and you know, like that. And what you want to do is after a time point t plus one, you want to predict the value for each of them. So how do you predict them? So maybe this time series depends on this one and this one also, and same way, you know, the red time series also, obviously its value, next value depends on previous values here, but also depends on the previous value for other time series. How do you capture these dependencies? So RNNs are a nice way of capturing those dependencies.
[11:54:23] are a great way of capturing these dependencies across multiple time series values. Therefore, we'll talk about RNNs and we'll understand how these are captured. In fact, to think about it, if I want to really forecast in a multivariate time series form, the way I can put this problem is as follows. Let's say I have like five different values at time point one. five different values at time point two, five values at time point three, five values at time point four, five values at time point five and maybe on the sixth
[11:54:57] time point, I want to predict what are these five values for each of the time series, each of the time series, right? So basically I have a vector, I can say I have a vector of values at each of these time points, and using this vector, let's call this vector as X, X1, X2, X3, X4, X5, I want to predict the vector at time point six. Makes sense? So where the vector could basically contain the values observed at that time point for different time series. So I want to basically figure out the value.
[11:55:22] value at time 0.6. Obviously the entire vector. Any questions so far? All good? I'm basically just going to talk about a new model which can help us do this. And that model is actually called as RNNs, so recurrent neural networks. So to make it easier to understand right, think about this vector as a word, as a word
[11:55:46] embedding. So now here is an important switch that I'm doing. I'm moving from the time series world to a text world. So therefore, this is very important. Please pay attention. I'm saying that I just showed to you how you would do predictions in a multivariate time series. How would you do that? For every time point, you have a vector of numbers. So if you have like 10 different time series that you want to
[11:56:12] track or forecast together, then you have at every time point you have a 10 sized vector and you want to predict at the next time point. An equivalent problem is to predict the embedding for the next word in a next word prediction problem. So rather than thinking about time series, now you could think about next word prediction. WhatsApp, when you type happy wedding, right?
[11:56:35] What does it predict? Anniversary for you, right? So you don't need to type it, basically just shows anniversary there. So when it shows that there must be some predictive mechanism and it's also a forecasting mechanism because you're sort of forecasting the next word. And this notion of forecasting the next word is very similar to the notion of forecasting time series values. In fact, in time series, literally the
[11:56:59] order is by time, in next word forecasting or next word prediction, the order is basically the sequence. You want to predict the sixth word, so to say. I could use the same thing that I've drawn here on paint and I could say that, hey, given the first five words, can I predict the sixth word in somebody's message? And the point is that,
[11:57:18] these words to be understandable by neural networks, they have to be represented using vectors. So that brings the similarity in the two things that anyway, I was talking about multivariate time series where each time point is represented using a vector. It turns out that even in the text sequences, each word is
[11:57:37] actually represented using a vector, a vector of numbers. That vector is also called as an embedding. So therefore, even in the text next word prediction problem, you would just be predicting the next vector, which represents the next word in that senses. So therefore, the two problems are very similar, whether it is
[11:57:56] the multivariate time series prediction or the next word prediction, they are both very similar problems. I'm transferring from the time series space to the text space just because it is much easier to understand RNNs in the text world. It is much easier to teach, it is much easier
[11:58:13] understand how RNNs work when you think about it as a next word prediction problem rather than a next time series multivariate time series prediction or forecasting problem. It's much easier to talk. So let's think about it as a next
[11:58:33] next word prediction problem. So you see, let's say you have like three words and you want to predict the fourth word. You have three words and you want to predict the fourth word. If I were to solve it in a multilayered perceptron kind of a manner, the way I would do that is as follows.
[11:58:54] I would give my first word, second word, third word. So I would give x1, x2, x3 and I would probably build a hidden layer and then I would make it predict the fourth word x4. So the idea is that I basically feed in this x1, x2, x3 and
[11:59:18] make it predict the x4, I mean the fourth word, okay? How do I make it predict the fourth word? Well, I mean there will be a bunch of neurons, maybe every word is being represented using 100 numbers, so well, there will be like 100 neurons in this final output layer and I'm going to make it predict the values of those 100 neurons, okay? The outputs of those 100 neurons, okay? And let's hope that
[11:59:39] the output matches with x4, which basically is great. If it doesn't match x4, well, I'll do back propagation and update these weights. I'll do a back propagation and update these weights. That's that. However, with this MLP, there is a problem. The problem is that MLP, this hidden layer is just observing all the words x1,
[12:00:06] x1, x2, x3 together. It is basically observing all the words x1, x2, x3 in parallel. But if you think about the human brain, human brain does not work this way. As a human we read the first word and then we think about the second word given the thought that we had after reading the first word. So we read in a
[12:00:32] manner from left to right. We basically read sentences from left to right. We continue to update the meaning that we have in our mind as we see the next word from left to right. We don't see all the words together. If the paragraph had like 50 words, we can't see all 50 together. We read the first one, then the second one, then the third one, then the fourth one and so on. So therefore, people thought that if one were to
[12:00:56] to do an appropriate time series forecasting or an appropriate next word prediction. As I said, both problems are very similar. One needs to be able to read these words from left to right and start making sense out of them. So that is why people came up with this network architecture. The way it works as follows. You start by reading the first word, you read the first word and you have a hidden layer,
[12:01:16] or meaning out of this first word. Then when you get the second word at the next time point, you actually read the second word, extract the features out of it using this hidden layer. But this hidden layer interestingly also takes care of the meaning so far of the thought that you have had so far. So if I am reading a sentence,
[12:01:47] Narendra or rather the first word in the sentence is Narendra. When I read the word Narendra, I can have several thoughts in my mind. I am not sure if some of you know Swami Vivekananda's first name was also Narendra. When I am just reading the sentence with the first word Narendra, I have many thoughts in my mind. It could be Narendra, Damodara,
[12:02:23] Das Modi or it could be Narendranath, I mean Swami Vekananda Gaya and so on. I don't know what it is. But it is some Narendra and it could also be somebody called Narendra at home. Who knows? So, when I read the second word and the second word is Modi, then I know which Narendra you are talking about. So, I take my thought so far and update it based on what I understood from the word Modi. I mean I understand some things independently from the word Modi also. Like it could be Nirav Modi.
[12:03:05] could be some other Modi as well, right? But well, I observed Narendra and I had some thought and I'm going to update that thought based on Modi. That's my updated thought. Now, you know, next word I read is, is, okay? So maybe the next word is, is, the word is. So I say, okay, is, is a word, that's great. So Narendra Modi is, so I've updated my thought, okay? And maybe the next word is, you know, X4, I read, the, okay? Great. So I read the word the and I have a thought so far and now I have to predict what is my fifth word going to be.
[12:04:10] Now, yeah, I mean, maybe, you know, the fifth word could be prime minister. So the next word could be prime. So Narendra Modi is the prime. Yeah, I mean, it makes more sense. So therefore what I do is I take the thought so far and try to use that thought so far connected with an output layer and use it to predict the next word prime. Okay. So that's that. So the idea in RNN is to be able to, you know, consume the words from left to right in a sequential manner as a human would do. and keep updating the thought. The thought so far is like short-term memory, you know, it's like memory. And that memory, I mean, that thought or memory is
[12:05:21] represented by the outputs of these hidden layers. So if you have these hidden layers, your thought or memory is really represented by these hidden layers themselves. You have one hidden layer per word in that census. So imagine if you're trying to process a paragraph of 100 words, then you would have like 100 hidden layers. So that's how these RNNs work. Think about these RNNs as like deep networks, which have like one hidden layer per word, and they're trying to process CAF such that it mimics the human way of processing sequences. And you can think about these guys in both the text space or in the time series space because both of them are sequences, algorithms deal with sequences. And they try to forecast the next thing in the sequence, whether it is a word in the sequence or it is a vector in a multivariate type series sequence, whichever way. I'll pause for questions here. There is way more things to talk about RNNs. I've actually shown you what is also there on the slide here. But I'll pause here for questions before I move further.
[12:06:40] We will move further. There is more stuff to it. But if there are questions, I'll take those questions first. I don't see anything in the chat window so far. But yeah, I think there are questions now. Let's see. RNN layers are equal to the number of input tokens. Yes, Yoganthar. So the question is, are RNN layers equal to number of inputs? Yes. I mean, if my input time points, it's equal to the number of time points, Yoganthar, more like. So if my input time points or the input number of words in the workspace are five, then I'll have five RNN layers. If the number of inputs are three and I'm going to make a prediction of the fourth word, then it is basically three hidden layers. Yeah, the number of hidden layers is going to be equal to the number of time points in the input. Yeah. Okay. So there is no any other decision layers to pre-process after the inputs? Like we have five samples we have taken. Then the meaning is we have extracted till we have to do more femoralized with
[12:09:25] those words and everything we have to do any like in CNN we have this feature extraction and along with the decision right like that we have any layers after the feature extraction like this. After the feature extraction there's just one output layer to basically extract to basically output the next word and this is also this is also our layer right so yeah you do have a final decision-making layer in that sense. Awesome. More questions. Venkat Ashok is asking, do we have back propagation RNN also similar to CNN? Absolutely. So you see every neural network, whatever it be, I mean, you know, if it is transformers or GPT or whatever it is, they will always be trained using back propagation. Back propagation is the mother algorithm of everything. Okay. So you will always have backpropagation. So here the idea is that if you do not predict the right next word, well, it is going to backpropagate and upgrade the weights for all the layers. So basically, yes, there is backpropagation. Penakin is asking, is same hidden layer used? It appears that RNN has more parameters than MLP. Am I correct? If yes, I guess it would be performance intensive. Thank you so much for those nice questions, Pinnakin. The first question is same hidden layer used. I think you meant to ask is the hidden layer size the same?
[12:13:51] in each of these hidden layers. So the answer is yes, the hidden layer size is the same. So if you are using H equal to 100, 100 neurons for the first hidden layer, you need to use 100 only in the next few hidden layers. Each hidden layer must use exactly the same size. And your second observation is also accurate, that it appears that RMS have more parameters than MLP. How would you solve this problem? Especially if you basically have like 100 different you know, words and you're trying to predict the 101st word, how would you solve this problem? Because it looks like you'll have too many parameters. So therefore, let me give you an interesting next step now. So the interesting part is that in RNNs, weights are shared. So which weights are shared though? Let's be careful about saying weights are shared. Which weights are shared? So you see this particular layers that I'm depicting now, they're all trying to extract meaning out of a word. They're all trying to extract the meaning out of a word. And therefore, these weights are shared. So this weight matrix is set as W1 for each of those layers. So for each of those layers, these weight matrices are all set to the same shared value. Similarly, if you notice these horizontal arrows, they are trying to just update your thought based on the current word. and the previous thought. So they are trying to update the thought based on the current word and the previous thought. So therefore these weights are also shared. They are shared and sent to W2. So they are the same weight matrix after all. However, this last guy W3 is a weight matrix by itself because well, it cannot be shared with anything else. So that's basically written as W3. So that's a third weight matrix. So you see, I mean, this makes my diagram much closer to what you see on the slide. I mean, it's not completely close. I mean, because some weights are not being also labeled here. But the whole point is that this is the full diagram, you see. You know, the weights are not going to be unique by themselves. They are shared and that basically ensures that the number of weights are much lower in this recurrent neural network compared to an MLP because the weights are shared. Now, you see, I mean, since the weights are shared, interestingly, I can process sequences of different lengths using the same neural network. Although the number of hidden layers are the same as the number of input words, if I have like a fifth word or rather I am processing an input of five words, so then I can very quickly add another input.
[12:19:28] input and another layer here because all I need are the weights. So this weight matrix is just going to be W1, this weight matrix is going to be W2 which basically means that at test time I can take inputs of different lengths and still come up with the prediction using the same network because after all the weights are shared at every time unit. At every time unit, the network behaves exactly identically as the next time unit, you see. So in every time unit, the network is doing the same thing. It's taking the current word XI, applying the weights W1, getting, so it's basically taking XI. Let me actually write it out nicely, okay? So what is the network doing at every time unit? At every time unit, it is basically taking my x t time unit t okay applying the weights w 1 to it okay applying the weights w 1 it's also taking the let me call this one as h t minus 1 hidden layer output at time point t minus 1 okay so it's taking the hidden layer output at time point t minus 1 applying the weights w 2 to it you know adding it up and then applying some sort of an activation function. It could be a 10 hyperbolic or a sigmoid activation function, and that is the output of the hidden layer. So what is the output of every hidden layer? Well, the output of every hidden layer is basically dependent on the output of the previous hidden layer. You multiply by the weight matrix W2, and you take the current word which has come in, Xt, this guy, and you multiply it with W1. And that is what each of the neurons in this hidden layer is doing. That is what each of the neurons, 100 neurons in this hidden layer is doing. And finally, it's basically applying some activation function because the neuron must apply an activation function. And coming up with output HT. Coming up with output HT. So this is what every layer, every hidden layer in the neural network is doing except for the last guy. The last guy basically is not doing this, right? Last guy is applying W3 to the output of the final hidden layer. So the last guy is basically applying W3 on the output of the final hidden layer. Let's call it H capital D, okay? And then maybe apply some activation function like softmax and come up with the final output and come up with the final output. But since each of the hidden layers is doing this kind of stuff which I showed at the bottom since each of the hidden layer is doing this kind of stuff which I am showing in this bottom thing you know it's called RNN okay because you see HT is expressed in terms of HT-1 there is recursion or recurrence here also okay this recursion or recurrence and that is why it's called recurrent neural network that is why it's called recurrent neural network. I'll take questions. I know there are questions. I'll take questions. But let me just look at this slide and help you understand the things that are written here. RNNs tie the weights at each time step. Tie in the sense, share the weights at each time step. They condition the neural network on all previous words. The next word depends on all previous words because the hidden layer at any time point captures the information that was encoded via these hidden layers. So it captures the information about all previous words. And you know how much RAM you would need only scales with the number of words. So if you had like 100 words, sure you'll need more RAM because you have to compute the hidden layer outputs for all 100 words. But the good point is that the same network can be used for inputs of 100 words, for inputs of 5 words, 10 words, whatever is the input length, it does not matter because the weights are all shared. If I were to really represent the network, right? I can represent it using a very, very simple way. I could say that the network at any time point takes XT, it processes it using this hidden layer, which basically takes your XT as input. But you know what? It also takes your HT minus one as input. And only at the last thing, it basically comes up with the final output. So if you really ask me, what is the network doing?
[12:25:22] network can be represented using just one hidden layer. Although it has like one hidden layer per input word, but they all share weights, the weights w1 and w2. Only the last layer has this w3 in it, but otherwise all the hidden layers basically share the weights w1 and w2. So therefore in a compressed form, I can show the network like that. and it can deal with all kinds of variable sized inputs. So some sample may have three words and I'm trying to break the fourth word. Some sample may have 10 words and I'm trying to break the 10th word, 11th word in that sense. Okay, let me pause for questions. There are more questions. So, but hopefully that answers your thinking again that you asked long back. Srivasa is asking, is it possible to know which input has given high weightage to prediction? Oh, good point, you see. You see all inputs give good weightage to the final prediction because the idea is that and rather I would say that all of them give more or less like equal weightage because you are using the same weight matrix W1 to look at every input. You use the same weight matrix W1 to look at every input and then it extracts semantics. So out of those words, you see I'm using this phrase called extract semantics. So extracting features is like extracting semantics. So it's trying to extract semantics or extract features out of these words with the same weight matrix W1. So in some ways it may look like it is giving equal importance to every word in some ways. Right, Srinivas? However, you see, I mean, if I look at more recent words, if I'm trying to make a prediction for the fifth word, if I look at the last word, whatever it is, if I look at this fourth word, it looks like it's going to give more importance to that because, you know, the semantics which are extracted from this last word are going to be directly used to make this prediction. But for others, let's say the 0th word that came in, you know, way back in the past, it's going to get multiplied by this W to weight matrix several times. It's going to get multiplied by this W to weight matrix several times. Its knowledge is only going to be indirectly used to come up with the final prediction. It's going to be only indirectly used to come with the final prediction. So it does look like, well, it's going to be more important to more recent words, which is reasonable several times, but sometimes it causes problems. One of the problems, the technical name for that problem is called vanishing gradient problem or the exploding gradient problem. So I'm trying to give a very full answer to your question. I could have simply just said, I'm using the same matrix W1, so it is giving equal importance to all words, but that's not true. The full answer is that, well, it's extracting stuff using W1, but on the way, it's also multiplying by this W2, which basically means that your past information, whether it is going to be given higher importance or lower importance depends on W2. So and depending on that, it actually causes a problem. In fact, that problem is called as the vanishing gradient problem or the exploding gradient problem. Sometimes you want, you know, passwords to have a significant impact on your current prediction. Sometimes you don't. Okay. So let me give you an example. Narendra Modi is the prime, you know, and you have to predict minister, you know. So, if somebody already stole Prime, you know, you want to predict the word Minister, it's not very complicated. I mean, you may not want the entire Nathindranath Modi thing, man. I mean, you just saw Prime and that's more or less good enough for you to predict Minister, okay? So, basically, sometimes short-term information is enough to come up with the final prediction. But, you know, consider another scenario, okay? Spain and France have been awesome friends for a long period of time. But in 1775, there are two. And I stop here. I want you to predict the word countries. To predict the word countries, you need to go back a fast past in time. And you want to use this information, Spain and France, which is way back in time. So sometimes it turns out that you don't want to depend a lot on the past. But sometimes you want to depend a lot on the future on the on the on the past past on the very remote past. Okay. So therefore this model unfortunately cannot capture that that kind of a notion. It has memory but it has uncontrolled memory unlike humans. So humans have controllable memory in the sense that if you want to remember, you know, you may still remember what happened five years back in time on your birthday. But you probably don't remember what which color T-shirt did you wear five days back in time. Essentially, it just depends on what we want to store in our permanent memory, in our long-term memory as it is called. So unfortunately, this guy does not have that long-term concept at all. It applies the same notion to everything. It doesn't have a way of storing stuff in long-term memory versus short-term memory. It only has short-term memory. It basically applies the same kind of a philosophy to every memory, to every input that comes in and it keeps storing it in the memory. So that is a drawback of this guy. But that is the long answer to your question, Srinivas. I mean, you know, to every input, it's doing the same thing, whether depending on the properties
[12:31:11] this W to weight matrix, either it remembers only the recent inputs or only the past inputs. So by giving very high weightage to that. So, but there are ways of addressing this issue by having a long term memory and that is what leads to another kind of networks called as LSTMs. They are called as LSTMs, long short term memory. They are called as long short term memory, but that is beyond the scope of this class. I'll probably not go into those details given that we are almost close to ending as well. Yoganthar has a question and I'll take that question quickly. Can we sync and expand same RN network based time series interval changes? Oh, very complicated for me to understand here. I don't know what you mean. Can we sync and expand same RN network based time series interval changes? With time series of 5 volts or the 100 volts? Can we use the same network to predict with the 3-word input and 100-word input? Yeah, absolutely. That was the question, yes. Because this network basically has the weights as repeated every time point. I mean, it just uses the same weights. Therefore, as I was saying, you can actually use it whether in your time series it has 3 as a size or 100 as a size. It doesn't matter. Until you want to make the next time point, you can make use of variable length time series, actually. Samples with variable length. OK, awesome. Taran has a question. If we slice a block of R9 to create a block for X0, how will that affect accuracy of prediction? Since W2s of header list will be based on how much memorization was used when training. Great question, Taran. Yeah, I will answer your second part also. W1 also probably affects since back propagation will update W1s to reduce error, which is contributions from W2 too. Yeah. OK. Yeah. The second one is easier. When you do backpropagation, you update both W1 and W2. In fact, when you do backpropagation in RNNs, it's actually called slightly differently. It's called backpropagation through time because you are going to backpropagate all the way through all the hidden layers and update their W1s and W2s. Now, while updating what happens is that your W1s and W2s across hidden layers may get updated in different ways. But that's fine. You know, after an entire backpropagation update is done, you basically take all of those W2s, take their average and set the average as W2, okay? Because you have to constrain all those W2s, right? So that's how you maintain a constrained shared weights across layers. But the other part of the question was, hey, at train time, if I only train with samples such that I am given four words and I want to predict fifth word, I am given four words, I want to predict the fifth word, then can I use this still for samples where, you know, I'm given three words and I want to predict the fourth word and I'm given 10 words and I want to predict the 11th word, is that reasonable? Yeah. So Taran Dage is that at train time itself, you try to have samples such that you have different number of words in your samples. So your first sample could have like three words and you'd want to print the fourth word. So at train time, you could actually have different number of words in your samples, which means that for different samples, you can actually create a network of that size and then you compute the prediction, compute the loss, backpropogate that loss, compute w1, w2, and that's it you want. You don't care about how many hidden layers were there, right? Because you don't store those inner layers anyway. You only store W1, W2, W3. That's all you care for. So at train time also, you have samples of variable sizes, avoiding this train test mismatch that you have in your mind. Okay, Saurav has a question, is Adhanan at every layer predicting outputs with different probabilities and next layer taking those probabilities of words to come up with new probabilities? Yeah, Saurav, rather than thinking about them as probabilities, think about them as thoughts so far. Okay. You're right. I mean, I can go with what you are saying, but you know, that's not the best way of thinking about it. Okay. The best way of thinking about it is that at every layer, it is basically just having some thought. And the point is that that thought is not the same as probabilities that need to be output because even at the end, you see this part is not the probability that is output. You know, this part needs to be converted using W3 to the probability of outputting a word. The way to really think about it is the hidden thought so far. So basically think about it like this. When you see a word as a human, there is some region in your brain which basically captures the semantics. So think about that hidden layer as the output of that region. So basically it's a thought so far and when you see the next word, you just keep updating the thought so far without the intention of outputting the next word. Okay. Yeah. I mean, you know, one of the intention is to output the next word, but the main intention is to understand what has happened so far. And that is why I'm calling it thought so far. Okay. So, the thought is basically the holistic understanding of whatever has been read so far. One of the tasks to do based on that is to output the word, but that's not the only thing. I mean, understanding is important whether you output or not. Sharad Chandra is asking, what are vanishing and exploding gradients and why are RNNs particularly susceptible to them? Yeah. So Sharad Chandra, I think I tried to answer that vanishing or exploding gradients happen because these W2s are the same things. I mean, if you basically capture some information from the word X0, you're going to multiply that information by the same W2 several times. So if you multiply any number,
[12:37:02] let's say if you have like 5 lakhs in your bank, you put them in an FD, which gives you 10% rate of interest, right? So, you know, you keep multiplying it over 100 years, you'll get a very large number that's called exploding gradients in some ways. If you multiply it with a number less than one, and you multiply it 100 times then you basically that 5 lakhs that you invested will become like 0 lakhs after like 100 years because you are multiplying with the number slightly less than 1 it's vanishing with some slightly less than 1 kind of a magnitude but then over 100 years it will vanish to 0. That's called vanishing gradients in some ways, very very crudely. But that's what is going on here. Since you are multiplying by the same value, same matrix, the information that you have from the past either gets magnified or exploded in that sense or vanishes. So that is what is called as the vanishing or exploding gradient problem. And, you know, RNNs are more susceptible to them because you are actually using these shared weights. So while shared weights is a blessing in some ways, you know, it also occurs because, you see, you lead to vanishing or exploding gradient problem. And LSTMs help solve that problem. Now that problem is not very significant when you're dealing with very small sequences in the sense that if you have just three words, you're trying to read the fourth word, sure, I mean, it doesn't vanish or it doesn't really explode. It just gets multiplied by some factor, okay? But if you're dealing with longer sequences, that becomes a trouble, okay? So if I want to read the next word in a paragraph, yeah, arguments are not going to be very good to do that. So that's that. Sarachandra is saying, what are some real world problems where RNNs excel? Great question Sarachandra, they excel when you are dealing with short sequences. So if you have like three word sequence WhatsApp, you know, RNNs are going to be good. Typically people don't type like very long, like hundred word messages on WhatsApp. I mean people do type, but then the next word prediction is not going to be very accurate if you use an RNN. Okay. So RNNs are good for short sequence predictions in that sense. Vinod has a question. Since you said RNNs are sequential, when backpropagation occurs, does it go back and it upgrades the weights? How can you say that it's sequential? Yeah, yeah, you see, yeah, people many times confuse sequential behavior with backpropagation, the backward pass in backpropagation. Let me try to disassociate the two and resolve the confusion. You see, I'm going to get away with this slide because to, or this drawing, right? And just create a new one, okay? So you see, RNs are sequential. And when I say RNs are sequential, I mean the following. They look at the first word, they learn information from the first word. Then they look at the second word, and then they'll update the information or update the meaning that they have learned so far. Then they look at the third word. Then they update the meaning and come up with the revised representation. Yeah, I mean, using this information, they try to predict the fourth word. If the fourth word is wrong, yes, backpropagation of information updates all of these weights. So backpropagation is going to, yes, update the weights in the backward direction. But those weights are just weight updates. They are different from the outputs. Remember, the outputs are always generated in the forward manner. Outputs are generated in the forward manner by looking at words one by one. Backpropagation updates happen in the backward manner, but that doesn't mean that the reasoning happen in the backward manner. In fact, after your entire training is done, you are going to do reasoning in the forward manner only because you're not going to do backpropagation at inference time, you see. So therefore, you know, the way the forward pass is happening is what matters. And that one is actually looking at words one by one in a sequence as a human would read them. Okay. Hopefully that answers your question, you know. Yeah. Ram has a question. Ramchandra has a question. Why can't we use transformers instead of RNNs? Great question Ramchandra. What are the advantage we will get compared to transformers? Can you explain with simple example how transformer versus RNN behave? Great question Ramchandra. Transformers are a heavyweight model. RNNs, basically, if you train them, they will have a number of parameters in a few millions. Transformer models, number of parameters are at least in a few tens of millions, if not billions. If you think about ChadGBD kind of models, they are like billions of parameters, very huge parameters. So transformers cannot be used in low latency applications. For example, you know, you go to Bing or Google, whatever, right? And you basically try to do some search, okay? So let's say I'm searching. Yeah, I mean, you see, I just searched for Virat Kohli and then it shows me all of those dropdowns, IPL, 225, images, YF and so on. This is like given these two things and I'm trying to type, it's basically trying to essentially come up with predictions, okay? So it's like next word prediction or it does multi-word prediction also. Here, I can't deploy a transformer model because transformers are going to be super slow. While I have to compete here with user's typing speed. So, you know, every time I type a character, I have to show those predictions and those predictions have to change faster than user's typing speed. And that's not possible using transformers because transformers are super slow. Even if you do significant optimizations, you know, it is still going to take much more than 15 to 20 milliseconds, which I have to actually compute and render the suggestions while incurring the network latency and everything. So in such applications where latency is a trouble, you can

# Session ended: 2026-02-08T12:41:44.039878
